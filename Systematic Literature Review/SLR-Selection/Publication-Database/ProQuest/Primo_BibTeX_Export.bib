@misc{SiposRuben2014CiFs,
abstract = {The main focus of this dissertation is new and improved ways of bringing high quality content to the users by leveraging the power of machine learning. Starting with a large amount of data we want to condense it into an easily digestible form by removing redundant and irrelevant parts and retaining only important information that is of interest to the user. Learning how to perform this from data allows us to use more complex models that better capture the notion of good content. Starting with supervised learning, this thesis proposes using structured prediction in conjunction with support vector machines to learn how to produce extractive summaries of textual documents. Representing summaries as a multivariate objects allows for modeling the dependencies between the summary components. An efficient approach to learning and predicting summaries is still possible by using a submodular objective/scoring function despite complex output space. The discussed approach can also be adapted to unsupervised setting and used to condense information in novel ways while retaining the same efficient submodular framework. Incorporating temporal dimension into summarization objective lead to a new way of visualizing flow of ideas and identifying novel contributions in a time-stamped corpus, which in turn help users gain a high level insight into evolution of it. Lastly, instead of trying to explicitly define an automated function used to condense information, one can leverage crowdsourcing. In particular, this thesis considers user feedback on online user-generated content to construct and improve content rankings. An analysis of a real-world dataset is presented and results suggest more accurate models of actual user voting patterns. Based on this new knowledge, an improved content ranking algorithm is proposed that delivers good content to the users in a shorter timeframe.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781321372809},
year = {2014},
title = {Condensing information: From supervised to crowdsourced learning},
language = {eng},
author = {Sipos, Ruben},
keywords = {Information Science ; Artificial Intelligence ; Computer Science ; Information Science ; Artificial Intelligence ; Computer Science ; Communication and the Arts ; Applied Sciences ; Condensing Information ; Crowdsourcing ; Information Retrieval ; Machine Learning ; Summarization ; User-Generated Content},
url = {http://search.proquest.com/docview/1634290345/},
}

@misc{PanditaRahul2015ISIf,
abstract = {Specifications play and important role in software engineering for ensuring software quality. Not only do the specifications guide the development process by outlining what/how to reuse, they also help in the verification process by allowing testers to test the expected outcome. For instance, static and dynamic program analysis tools use formal specifications such as code contracts or assertions to detect violations of these specifications as defects. Likewise, API migration tools use predefined mapping specifications to perform automated migration. While highly desirable, oftentimes such formal specifications are missing from existing software. In contrast, these specifications are described in natural language in various software artifacts. Particularly, Application Programming Interface (API) documents that are targeted towards developers, are invaluable source of information regarding code-level specifications. However, (most of) existing developer productivity tools/frameworks are not designed to process the natural-language descriptions in software artifacts. The goal of this work is to improve developer / tester / end-user productivity by accurately identifying specification sentences from the natural language text in software artifacts and representing them formally through adaptation of existing text analysis techniques. Since natural-language software artifacts are often verbose, manually writing formal specifications from software artifacts may be resource intensive and error prone. To address this issue, this dissertation presents a text mining and natural language processing (NLP) framework to automate the task of inferring semantic information from natural-language in software artifacts to bridge the disconnect between the inputs required by software engineering tools/frameworks and the specifications described in natural-language software artifacts. Specifically, this dissertation reports on the process and effectiveness of applying text mining and NLP techniques in the following developer problems: 1. Specification of a method arguments and return values describes how to use a particular method within a class in terms of the expectations of the method arguments (preconditions) and expected return values (post-conditions). We propose a novel approach to infer these specifications from natural-language text of API documents. Our evaluation results show that our approach achieves an average of 92% precision and 93% recall in identifying sentences that describe such specifications from more than 2500 sentences of API documents. Furthermore, our results show that our approach has an average 83% accuracy in inferring specifications from over 1600 specification sentences. 2. Temporal specifications of an API are the allowed sequences of method invocations in the API. We propose ICON: an approach based on machine learning and NLP for identifying and inferring formal temporal constraints to assist tool based verification. Our evaluation results indicate that ICON is effective in identifying temporal constraint sentences (from over 4000 human-annotated API sentences) with the average precision, recall, and F-score of 79.0%, 60.0%, and 65.0%, respectively. Furthermore, our evaluation also demonstrates that ICON achieves an accuracy of 70% in inferring and formalizing 77 temporal constraints from these temporal constraint sentences. 3. API mappings facilitate tool based languagemigration.We propose TMAP: TextMining based approach to discover likely API mappings using the similarity in the textual description of the source and target API documents. We evaluated TMAP by comparing the discovered mappings with state-of-the-art source code analysis based approaches: Rosetta and StaMiner. Our results indicate that TMAP on an average found relevant mappings for 57% more methods compared to previous approaches. Furthermore, our results also indicate that TMAP found, on average, exact mappings for 6.5 more methods per class as compared to previous approaches. 4. Keeping malware out of mobile application markets is an ongoing challenge. To assist third-party testers for assessing risk of mobile applications, we present WHYPER, a framework using NLP techniques to identify sentences that describe the need for a given permission in an application description. WHYPER achieves an average precision of 82.8%, and an average recall of 81.5% for three permissions (address book, calendar, and record audio) that protect frequently-used security and privacy sensitive resources. These results demonstrate great promise in using NLP techniques in aiding the risk assessment of mobile applications.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781339760360},
year = {2015},
title = {Inferring Semantic Information from Natural-Language Software Artifacts},
language = {eng},
author = {Pandita, Rahul},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Application Programming Interface Documents ; Natural Language ; Software Engineering ; Specifications ; Text Mining},
url = {http://search.proquest.com/docview/1798874981/},
}

@article{TabarceaAndrei2017Ffls,
issn = {1748-9725},
abstract = {Abstract Nowadays, a large part of the multimedia data on the Internet is generated with devices that automatically annotate them with location information However, free-form content on websites does not implicitly contain any geographical information. This is the biggest challenge for building a location-aware search engine. In this paper, we study how to extract location-aware information from the web. The key challenges are to detect location from a web page and to extract relevant information related to that location. We detect locations by identifying postal addresses using freely available gazetteers. Additional information for summarising the search results are titles and representative images, which we mine from the content using simple rule-based approaches utilising the structure of web pages. This information can be used to personalise search results for mobile users so that the results are relevant to their location.},
journal = {Journal of Location Based Services},
pages = {50--74},
volume = {11},
publisher = {Taylor & Francis},
number = {1},
year = {2017},
title = {Framework for location-aware search engine},
language = {eng},
author = {Tabarcea, Andrei and Gali, Najlah and Fränti, Pasi},
keywords = {Article ; Location-Based Search ; Address Detection ; Web-Mining ; Prefix Trees},
}

@misc{VanEgmondEric2014CSPi,
abstract = {Many web search improvements have been developed since the advent of the modern search engine, but one underrepresented area is the application of specific customizations to search results for educational web sites. In order to address this issue and improve the relevance of search results in automated learning environments, this work has integrated context-aware search principles with applications of preference based re-ranking and query modifications. This research investigates several aspects of context-aware search principles, specifically context-sensitive and preference based re-ranking of results which take user inputs as to their preferred content, and combines this with search query modifications which automatically search for a variety of modified terms based on the given search query, integrating these results into the overall re-ranking for the context. The result of this work is a novel web search algorithm which could be applied to any online learning environment attempting to collect relevant resources for learning about a given topic. The algorithm has been evaluated through user studies comparing traditional search results to the context-aware results returned through the algorithm for a given topic. These studies explore how this integration of methods could provide improved relevance in the search results returned when compared against other modern search engines.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781321384505},
year = {2014},
title = {Context-Aware Search Principles in Automated Learning Environments},
language = {eng},
author = {Van Egmond, Eric},
keywords = {Educational Technology ; Computer Science ; Educational Technology ; Computer Science ; Applied Sciences ; Education ; Context-Aware Search ; Learning Environments ; Web Search},
url = {http://search.proquest.com/docview/1640883599/},
}

@misc{AlnusairAwny2010Spua,
abstract = {Program understanding is vital for enhancing and maintaining existing software systems as well as building new systems from libraries of reusable program code. Unfortunately, the process of understanding is often time consuming, especially in legacy and open source software libraries. While some exceptional pieces of software may be documented well, it is often the case that libraries lack informative API documentation and reliable design information. Thus, leaving the source-code itself as the only form of reference for most program understanding activities. This dissertation contributes solutions that leverage program understanding and knowledge reuse. Firstly, in order to recover the lost design decisions, we developed an approach for reverse engineering design patterns from source-code. Secondly, we present an approach for automatic realization and retrieval of software components in large reuse libraries. In order to further maximize the reuse potential, we have also developed a source-code recommendation approach that automatically constructs and delivers relevant source-code snippets that can be used to complete particular programming tasks. In arriving at such solutions, we utilized ontological modeling to provide formal, explicit, and semantic representation of the conceptual source-code knowledge. This representation is the basis for computing entailments and enabling enhanced semantic reasoning support. The work presented in this dissertation has been implemented and validated by applying our program understanding tools to several large-scale software libraries. Our experimental evaluations show evidence that ontology formalisms combined with rule restrictions can effectively detect pattern instances without relying on hard-coded heuristics. It also show evidence that such ontology formalisms combined with context-sensitive techniques enhance precision when retrieving reusable code even without mining an existing corpus of sample code.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781124113432},
year = {2010},
title = {Semantics-enabled program understanding and knowledge reuse},
language = {eng},
author = {Alnusair, Awny},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Knowledge Reuse ; Semantic Web ; Software Engineering},
url = {http://search.proquest.com/docview/741298781/},
}

@misc{RongXin2017NLMf,
abstract = {Programming can be hard to learn and master. Search engines and social Q&A websites offer tremendous help to programmers, but great expertise (e.g., “Google-fu”) is required to efficiently use these resources and successfully solve complex problems. An integrated system that can recognize a programmer’s tasks and provide contextualized solutions is thus desirable, and ideally programmers can interact with the system using natural input channels, in a way similar to how they communicate with a human expert. To enable such an integrated system, neural language models constitute a promising solution. These models encode programming language in the same high-dimensional space with data of other modalities, and can be trained in an end-to-end fashion. By leveraging the massive data about programming knowledge that are available online, including social Q&A websites, tutorials, blogs, and open-source code repositories, we can train neural language models to support a variety of user intentions, including the long-tail ones. We propose three studies related to using neural language models to solve programming problems in practice. First, we introduce CodeMend, an intelligent programming assistant that supports interactive programming. The system employs a bimodal embedding model to encode programming language and natural language in the same vector space. We demonstrate that this model can effectively understand the code context and associate it with user input to suggest relevant code modifications. We also develop novel user interface to render search results in a way that makes the problem solving process more efficient. Second, we propose a deep learning pipeline that converts data visualization images to source code. The pipeline is built by using computer vision techniques and recurrent neural networks, and it supports the user to get source code generated based on visual examples. We develop novel techniques that augment existing a limited set of training samples via code parameterization and random variation. We also propose strategies that can adapt the general-purpose neural language model to fit the task of predicting source code. Third, we introduce LAMVI, a set of visualization tools for diagnosing issues with neural language models. It tracks the ranks of individual candidate outputs for user-selected queries, and supports the exploration of the corresponding hidden-layer activations. It also tracks influential training instances, and provides guidance for taking actions for tuning the model. The system is evaluated on simulated datasets facilitates the user to efficiently adapt mature neural language models to new datasets or new tasks. Collectively, these three components form an integral solution to computer-assisted problem solving for programmers driven by big data, and may have impact on various different domains, including natural language processing, machine learning, software engineering, and interactive data visualization.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780355366457},
year = {2017},
title = {Neural Language Models for Data-driven Programming Support},
language = {eng},
author = {Rong, Xin},
keywords = {Information Science ; Information Science ; Communication and the Arts ; Neural Language Models},
url = {http://search.proquest.com/docview/1990221328/},
}

@article{EckmanBarbara2001Etqi,
issn = {13674803},
abstract = {Motivation: To identify and characterize regions of functional interest in genomic sequence requires full, flexible query access to an integrated, up-to-date view of all related information, irrespective of where it is stored (within an organization or across the Internet) and its format (traditional database, flat file, web site, results of runtime analysis). Wide-ranging multi-source queries often return unmanageably large result sets, requiring non-traditional approaches to exclude extraneous data. Results: Target Informatics Net (TINet) is a readily extensible data integration system developed at GlaxoSmith- Kline (GSK), based on the Object-Protocol Model (OPM) multidatabase middleware system of Gene Logic Inc. Data sources currently integrated include: the Mouse Genome Database (MGD) and Gene Expression Database (GXD), GenBank, SwissProt, PubMed, GeneCards, the results of runtime BLAST and PROSITE searches, and GSK proprietary relational databases. Special-purpose class methods used to filter and augment query results include regular expression pattern-matching over BLAST HSP alignments and retrieving partial sequences derived from primary structure annotations. All data sources and methods are accessible through an SQL-like query language or a GUI, so that when new investigations arise no additional programming beyond query specification is required. The power and flexibility of this approach are illustrated in such integrated queries as: (1) 'find homologs in genomic sequence to all novel genes cloned and reported in the scientific literature within the past three months that are linked to the MeSH term 'neoplasms"; (2) 'using a neuropeptide precursor query sequence, return only HSPs where the target genomic sequences conserve the G[KR][KR] motif at the appropriate points in the HSP alignment'; and (3) 'of the human genomic sequences annotated with exon boundaries in GenBank, return only those with valid putative donor/acceptor sites and start/stop codons'. Availability: Freely available to non-profit educational and research institutions. Usage by commercial entities requires a license agreement. Contact: barbara_ eckman@sbphrd.com To whom correspondence should be addressed.},
journal = {Bioinformatics},
volume = {17},
publisher = {Oxford Publishing Limited(England)},
number = {7},
year = {2001},
title = {Extending traditional query-based integration approaches for functional characterization of post-genomic data},
language = {eng},
address = {Oxford},
author = {Eckman, Barbara and Kosky, Anthony and Laroco, Leonardo},
keywords = {Bioinformatics ; Genomes ; Computer Applications ; Computer Programs ; Bioinformatics and Computer Applications ; Miscellaneous, Reviews;},
url = {http://search.proquest.com/docview/198647018/},
}

@inproceedings{AgeevMikhail2013Isrs,
series = {SIGIR '13},
abstract = {<p><p>Query-biased search result summaries, or "snippets", help users decide whether a result is relevant for their information need, and have become increasingly important for helping searchers with difficult or ambiguous search tasks. Previously published snippet generation algorithms have been primarily based on selecting document fragments most similar to the query, which does not take into account which parts of the document the searchers actually found useful. We present a new approach to improving result summaries by incorporating post-click searcher behavior data, such as mouse cursor movements and scrolling over the result documents. To achieve this aim, we develop a method for collecting behavioral data with precise association between searcher intent, document examination behavior, and the corresponding document fragments. In turn, this allows us to incorporate page examination behavior signals into a novel Behavior-Biased Snippet generation system (BeBS). By mining searcher examination data, BeBS infers document fragments of most interest to users, and combines this evidence with text-based features to select the most promising fragments for inclusion in the result summary. Our extensive experiments and analysis demonstrate that our method improves the quality of result summaries compared to existing state-of-the-art methods. We believe that this work opens a new direction for improving search result presentation, and we make available the code and the search behavior data used in this study to encourage further research in this area.</p></p>},
pages = {13--22},
publisher = {ACM},
booktitle = {Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval},
isbn = {9781450320344},
year = {2013},
title = {Improving search result summaries by using searcher behavior data},
language = {eng},
author = {Ageev, Mikhail and Lagun, Dmitry and Agichtein, Eugene},
keywords = {Mouse Cursor Movement ; Result Summary Generation ; Searcher Behavior ; Library & Information Science},
}

@misc{SeoJangwon2011Susm,
abstract = {Social applications on the Web have appeared as communication spaces for sharing knowledge and information. In particular, social applications can be considered valuable information sources because information in the applications is not only easily accessible but also revealing in that the information accrues via interactions between people. In this work, we address methods for finding relevant information in social media applications that use unique properties of these applications. In particular, we focus on three unique structures in social media: hierarchical structure, conversational structure, and social structure. Hierarchical structures are used to organize information according to certain rules. Conversational structures are formed by interactions within communities such as replies. Social structures represent social relationships among community members. These structures are designed to organize information and encourage people to participate in discussions in social applications. Accordingly, contexts extracted from these structures can be used to improve the effectiveness of search in social media relative to representations based solely on text content. To exploit these structures in retrieval frameworks, we need to address three challenges as follows. First, we should discover each structure because it is often obscure. Second, we need to extract relevant contexts from each structure because not all the contexts in a structure are relevant for retrieval. Last, we should represent each context or their combinations in a representation framework so that they can be encoded as retrieval components such as documents. In this work, we introduce an effective representation framework for multiple contexts. We then discuss how to discover or define each structure and how to extract relevant contexts from the structure. Using the representation framework, these relevant contexts are integrated into retrieval algorithms. To demonstrate that these structures can improve search in social media, the retrieval models and frameworks incorporating these structures are evaluated through experiments using data collections gathered from a variety of social media applications. In addition, we address two minor challenges related to social media search. First, it is not always easy to find relevant information from relevant objects if the objects are large. Accordingly, we address identification of relevant substructures in such objects. Second, text reuse structures are important since these structures have the potential to affect various retrieval tasks. In this thesis, we introduce text reuse structures and analyze text reuse patterns in real social applications.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781267042569},
year = {2011},
title = {Search using social media structures},
language = {eng},
author = {Seo, Jangwon},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Forum Search ; Social Media Search ; Social Media Structures},
url = {http://search.proquest.com/docview/911024141/},
}

@inproceedings{ZilbersteinMeital2016Laco,
series = {Onward! 2016},
abstract = {<p><p> Program similarity is a central challenge in many programming-related applications, such as code search, clone detection, automatic translation, and programming education. </p> <p> We present a novel approach for establishing the similarity of code fragments by: (i) obtaining textual descriptions of code fragments captured in millions of posts on question-answering sites, blogs and other sources, and (ii) using natural language processing techniques to establish similarity between textual descriptions, and thus between their corresponding code fragments. </p> <p> To improve precision, we use a simple static analysis that extracts type signatures, and combine the results of textual similarity with similarity of the signatures. Because our notion of code similarity is based on similarity of textual descriptions, our approach can determine semantic relatedness and similarity of code across different libraries and even across different programming languages, a task considered extremely difficult using traditional approaches. To evaluate our approach, we use data obtained from the popular question-answering site, Stackoverflow. To obtain a ground-truth to compare against, we developed a crowdsourcing system, Like2Drops, that allows users to label the similarity of code fragments. We used the system to collect similarity classifications for a massive corpus of 6,500 program pairs. Our results show that our technique is effective in determining similarity, and achieves more than 85 percent precision, recall and accuracy. </p></p>},
pages = {197--211},
publisher = {ACM},
booktitle = {Proceedings of the 2016 ACM International Symposium on new ideas, new paradigms, and reflections on programming and software},
isbn = {9781450340762},
year = {2016},
title = {Leveraging a corpus of natural language descriptions for program similarity},
language = {eng},
author = {Zilberstein, Meital and Yahav, Eran},
keywords = {Code Similarity ; Natural Language ; Program Analysis ; Semantics},
}

@misc{StoleeKathryn2013Stsf,
abstract = {Programmers frequently search for source code to reuse using keyword searches. When effective and efficient, a code search can boost programmer productivity, however, the search effectiveness depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches either do not scale, are not flexible enough to find approximate matches, or require complex specifications. We propose a novel approach to semantic search that addresses some of these limitations and is designed for queries that can be described using an example. In this approach, programmers write lightweight specifications as inputs and expected output examples for the behavior of desired code. Using these specifications, an SMT solver identifies source code from a repository that matches the specifications. The repository is composed of program snippets encoded as constraints that approximate the semantics of the code. This research contributes the first work toward using SMT solvers to search for existing source code. In this dissertation, we motivate the study of code search and the utility of a more semantic approach to code search. We introduce and illustrate the generality of our approach using subsets of three languages, Java, Yahoo! Pipes, and SQL. Our approach is implemented in a tool, Satsy, for Yahoo! Pipes and Java. The evaluation covers various aspects of the approach, and the results indicate that this approach is effective at finding relevant code. Even with a small repository, our search is competitive with state-of-the-practice syntactic searches when searching for Java code. Further, this approach is flexible and can be used on its own, or in conjunction with a syntactic search. Finally, we show that this approach is adaptable to finding approximate matches when exact matches do not exist, and that programmers are capable of composing input/output queries with reasonable speed and accuracy. These results are promising and lead to several open research questions that we are only beginning to explore.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781303257391},
year = {2013},
title = {Solving the search for source code},
language = {eng},
author = {Stolee, Kathryn},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Code Search ; Program Analysis ; Semantic Search ; Software Engineering},
url = {http://search.proquest.com/docview/1427347505/},
}

@article{DeLuciaAndrea2013Aasf,
issn = {0950-5849},
abstract = {ContextTraceability relations among software artifacts often tend to be missing, outdated, or lost. For this reason, various traceability recovery approaches—based on Information Retrieval (IR) techniques—have been proposed. The performances of such approaches are often influenced by “noise” contained in software artifacts (e.g., recurring words in document templates or other words that do not contribute to the retrieval itself). AimAs a complement and alternative to stop word removal approaches, this paper proposes the use of a smoothing filter to remove “noise” from the textual corpus of artifacts to be traced. MethodWe evaluate the effect of a smoothing filter in traceability recovery tasks involving different kinds of artifacts from five software projects, and applying three different IR methods, namely Vector Space Models, Latent Semantic Indexing, and Jensen–Shannon similarity model. ResultsOur study indicates that, with the exception of some specific kinds of artifacts (i.e., tracing test cases to source code) the proposed approach is able to significantly improve the performances of traceability recovery, and to remove “noise” that simple stop word filters cannot remove. ConclusionsThe obtained results not only help to develop traceability recovery approaches able to work in presence of noisy artifacts, but also suggest that smoothing filters can be used to improve performances of other software engineering approaches based on textual analysis.},
journal = {Information and Software Technology},
pages = {741--754},
volume = {55},
publisher = {Elsevier B.V.},
number = {4},
year = {2013},
title = {Applying a smoothing filter to improve IR-based traceability recovery processes: An empirical investigation},
language = {eng},
author = {De Lucia, Andrea and Di Penta, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
keywords = {Software Traceability ; Information Retrieval ; Smoothing Filters ; Empirical Software Engineering},
}

@article{PaydarSamad2015Aswe,
issn = {0928-8910},
abstract = {Web engineering has emerged as a new software discipline to specifically address the challenges and complexities of developing high quality web applications. A main theme in different web engineering methodologies is to employ model driven development approaches. This increases the level of abstraction and formalism to the extent that machines can better involve in the development process and provide more automation, e.g. automatic code generation from the models. Despite their benefits, a main problem of these model-driven methodologies is that developing each new web application implies creating a probably large number of models from scratch. Hence, model reuse can be considered as the main solution to this problem. In this paper, a semantic web enabled approach is proposed for reusing models, specifically functional requirements models. It takes the brief description of the functional requirements of a new web application in terms of UML use case diagram, and semi-automatically generates the draft of the corresponding detailed description in terms of a set of UML activity diagrams. This is performed by utilizing a repository which contains semantic representation of the models of the previous web applications. The proposed approach is based on novel algorithms for annotating activity diagrams, measuring similarity of use cases, and adapting activity diagrams. The experimental evaluations demonstrate that the proposed approach is promising, and it has good precision and effectiveness.},
journal = {Automated Software Engineering},
pages = {241--288},
volume = {22},
publisher = {Springer US},
number = {2},
year = {2015},
title = {A semantic web enabled approach to reuse functional requirements models in web engineering},
language = {eng},
address = {Boston},
author = {Paydar, Samad and Kahani, Mohsen},
keywords = {Web engineering ; Semantic web ; Annotation ; Adaptation ; Use case similarity ; Reuse},
}

@misc{LiuHuixiang2002Istf,
abstract = {There are many tools available today to help software engineers search in source code systems. It is often the case, however, that there is a gap between what people really want to find and the actual query strings they specify. This is because a concept in a software system may be represented by many different terms, while the same term may have different meanings in different places. Therefore, software engineers often have to guess as they specify a search, and often have to repeatedly search before finding what they want. To alleviate the search problem, this thesis describes a study of what we call intelligent search techniques as implemented in a software exploration environment, whose purpose is to facilitate software maintenance. We propose to utilize some information retrieval techniques to automatically apply transformations to the query strings. The thesis first introduces the intelligent search techniques used in our study, including abbreviation concatenation and abbreviation expansion. Then it describes in detail the rating algorithms used to evaluate the query results' similarity to the original query strings. Next, we describe a series of experiments we conducted to assess the effectiveness of both the intelligent search methods and our rating algorithms. Finally, we describe how we use the analysis of the experimental results to recommend an effective combination of searching techniques for software maintenance, as well as to guide our future research.},
publisher = {ProQuest Dissertations Publishing},
isbn = {0612901106},
year = {2002},
title = {Intelligent search techniques for large software systems},
language = {eng},
author = {Liu, Huixiang},
keywords = {Computer Science ; Artificial Intelligence ; Computer Science ; Artificial Intelligence ; Applied Sciences},
url = {http://search.proquest.com/docview/305501281/},
}

@misc{ColvinErin2014Ufsf,
abstract = {This dissertation provides an analysis of the use of fuzzy sets for retrieval of software for the purpose of reuse. The need for quality software components is growing exponentially but the ability of software developers to meet this need is not. One major goal of any programmer is to develop quality software in an efficient amount of time. By reusing an already created and tested piece of code, programmers can do just that. Most development teams hesitate from the reuse realm because they cannot find quality software quickly. Software needs to be easily accessible and the results of a query to find quality software need to meet a user's expectation. Most software search algorithms are based on a Boolean search, where a term used to describe a given software component either matches or doesn't match a queried term. Here, using fuzzy logic, a term is given a weight based on its degree of membership, which will result in a weighted list of matches while maintaining the semantics of Boolean logic. The result is a list of matched documents in descending order by how well the document matches the queried term. Various methods of information retrieval implementation, analysis of fuzzy set retrieval, and benefits of its use for software reuse are examined and presented in this dissertation. A deeper explanation of the fundamentals of designing a fuzzy information retrieval system for software look up will also be examined. Future research options and necessary data storage systems will be explained as well. The results will show that the fuzzy methods did provide a better match to a user's query, with an increased mean average precision over a Boolean search by over 20%.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781321446593},
year = {2014},
title = {Using fuzzy sets for retrieval of software for reuse},
language = {eng},
author = {Colvin, Erin},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Boolean Search ; Efficient ; Fuzzy Sets ; Retrieval ; Reuse ; Software},
url = {http://search.proquest.com/docview/1648654045/},
}

@article{SonkarShailendraKumar2015NoIS,
issn = {1040-1628},
abstract = {The user of dynamic social network does not require irrelevant and vast amount of information during a search. A need of an intelligent search is required to get the reduced, filtered and relevant information that is achieved using an intelligent information retrieval and web mining. In this paper, identification and description of facts related to needs of an intelligent search in dynamic social network has been done by the authors after the deep and thorough study conducted on several journal and conference papers that are scattered on different electronic databases globally. The usage of intelligent agent for effective information retrieval from the social network site is a very emerging area and it will help the users to find the relevant and concerned information quickly and efficiently. The findings of the authors will help researchers and scholars who are already working in this area to get the relevant information in the direction of future research.},
journal = {Information Resources Management Journal},
volume = {28},
publisher = {IGI Global},
number = {2},
year = {2015},
title = {Need of Intelligent Search in Dynamic Social Network.(Research Article)(Report)},
language = {eng},
author = {Sonkar, Shailendra Kumar and Bhatnagar, Vishal and Challa, Rama Krishna},
keywords = {Online Social Networks – Technology Application ; Online Searching – Methods},
}

@article{SeoJangwon2011Ocsu,
issn = {1386-4564},
abstract = {Online communities are valuable information sources where knowledge is accumulated by interactions between people. Search services provided by online community sites such as forums are often, however, quite poor. To address this, we investigate retrieval techniques that exploit the hierarchical thread structures in community sites. Since these structures are sometimes not explicit or accurately annotated, we introduce structure discovery techniques that use a variety of features to model relations between posts. We then make use of thread structures in retrieval experiments with two online forums and one email archive. Our results show that using thread structures that have been accurately annotated can lead to significant improvements in retrieval performance compared to strong baselines.},
journal = {Information Retrieval},
pages = {547--571},
volume = {14},
publisher = {Springer Netherlands},
number = {6},
year = {2011},
title = {Online community search using conversational structures},
language = {eng},
address = {Dordrecht},
author = {Seo, Jangwon and Bruce Croft, W. and Smith, David},
keywords = {Online community ; Forum search ; Thread structure},
}

@article{LiZhixing2018Cssb,
issn = {2095-2228},
abstract = {Internet-scale open source software (OSS) production in various communities generates abundant reusable resources for software developers. However, finding the desired and mature software with keyword queries from a considerable number of candidates, especially for the fresher, is a significant challenge because current search services often fail to understand the semantics of user queries. In this paper, we construct a software term database (STDB) by analyzing tagging data in Stack Overflow and propose a correlation-based software search (CBSS) approach that performs correlation retrieval based on the term relevance obtained from STDB. In addition, we design a novel ranking method to optimize the initial retrieval result. We explore four research questions in four experiments, respectively, to evaluate the effectiveness of the STDB and investigate the performance of the CBSS. The experiment results show that the proposed CBSS can effectively respond to keyword-based software searches and significantly outperforms other existing search services at finding mature software.},
journal = {Frontiers of Computer Science},
pages = {923--938},
volume = {12},
publisher = {Higher Education Press},
number = {5},
year = {2018},
title = {Correlation-based software search by leveraging software term database},
language = {eng},
address = {Beijing},
author = {Li, Zhixing and Yin, Gang and Wang, Tao and Zhang, Yang and Yu, Yue and Wang, Huaimin},
keywords = {software retrieval ; software term database ; open source software},
}

@article{LiuZhiping2018CCPf,
issn = {1007-1202},
abstract = {When source code is over-specific to some concrete contexts, developers have to manually change the source code retrieved from the Internet. To solve this problem, we propose the context-aware change pattern (CACP). For a piece of source code, we extract the changes and changes-relevant context from the past code changes, identifying CACP that is the abstract common part of the changes and context. By using CACP, the retrieved source code could be transformed into the suitable one according to different user needs. From the Github we extracted 7 topics, collected 5–6 code snippets per topic and performed 5 different experiments which illustrated that CACP improves code transformation accuracy by 73.84%.},
journal = {Wuhan University Journal of Natural Sciences},
pages = {355--361},
volume = {23},
publisher = {Wuhan University},
number = {4},
year = {2018},
title = {Context-Aware Change Pattern for Code Transformation},
language = {eng},
address = {Wuhan},
author = {Liu, Zhiping},
keywords = {code transformation ; context-aware change pattern ; code change},
}

@inproceedings{WuNaihao2016Luti,
series = {CSI-SE '16},
abstract = {<p><p>Traceability links between software artifacts have important applications in the development process. This paper concerns a special case of traceability recovery, i.e., the automated integration of API usage tutorials with the API client code. Our solution involves partitioning the client code into multiple semantic groups/snippets and linking each snippet to the best matching tutorials. Evaluation using benchmarks created for two popular APIs reveals that our solution can find the expected tutorial links at the average rank of 1.6 and 1.4 in the top ranked results, for the two API's, respectively, and with good average precision and recall. We also evaluate the impact of both method partitioning and JavaDoc query expansion on tutorial linking performance. Lastly, we conduct a formative user study to pinpoint the scenarios where our solution actually helps a software developer. We conclude that it is a promising approach to speeding up the maintenance of API client code.</p></p>},
pages = {22--28},
publisher = {ACM},
booktitle = {Proceedings of the 3rd International Workshop on crowdsourcing in software engineering},
isbn = {9781450341585},
year = {2016},
title = {Linking usage tutorials into API client code},
language = {eng},
author = {Wu, Naihao and Hou, Daqing and Liu, Qingkun},
keywords = {Api ; Java ; Abstract Syntax Tree ; Documentation ; Information Retrieval ; Traceability ; Vector Space Model},
}

@inproceedings{IonescuBogdan2016Dasi,
series = {MMSys '16},
abstract = {<p><p>In this paper we introduce a new dataset, Div150Multi, that was designed to support shared evaluation of diversification techniques in different areas of social media photo retrieval and related areas. The dataset comes with associated relevance and diversity assessments performed by trusted annotators. The data consists of around 300 complex queries represented via 86,769 Flickr photos, around 27M photo links for around 6,000 users, metadata, Wikipedia pages and content descriptors for text and visual modalities, including state of the art deep features. To facilitate distribution, only Creative Commons content allowing redistribution was included in the dataset. The proposed dataset was validated during the 2015 Retrieving Diverse Social Images Task at the MediaEval Benchmarking.</p></p>},
pages = {1--6},
publisher = {ACM},
booktitle = {Proceedings of the 7th International Conference on multimedia systems},
isbn = {9781450342971},
year = {2016},
title = {Div150Multi: a social image retrieval result diversification dataset with multi-topic queries},
language = {eng},
author = {Ionescu, Bogdan and Gînscă, Alexandru and Boteanu, Bogdan and Lupu, Mihai and Popescu, Adrian and Müller, Henning},
keywords = {Mediaeval Benchmark ; Multi-Topic Queries ; Search Result Diversification ; Social Photo Retrieval ; User Tagging Credibility},
}

@inproceedings{ZhuDongqing2012Cmef,
series = {SHB '12},
abstract = {<p><p>The increasing prevalence of electronic health records containing rich information about a patient's health and physical condition has the potential to transform research in health and medicine. In this work, we present a health record search system for finding patients matching certain inclusion criteria (specified as keyword queries) for clinical studies. In particular, our system aggregates multi-level evidence and combines proven statistical IR models, both in an innovative way, and achieves a 20% MAP (mean average precision) improvement over a strong baseline. Moreover, our cross-validation results show that the overall performance of our system is comparable to other top-performing systems on the same task.</p></p>},
pages = {49--56},
publisher = {ACM},
booktitle = {Proceedings of the 2012 international workshop on smart health and wellbeing},
isbn = {9781450317122},
year = {2012},
title = {Combining multi-level evidence for medical record retrieval},
language = {eng},
author = {Zhu, Dongqing and Carterette, Ben},
keywords = {Ehr ; Evidence Aggregation ; Information Retrieval ; Language Models ; Medical Record Search ; Public Health},
}

@article{LykkeMarianne2012HdsA,
issn = {0306-4573},
abstract = {Highlights► Querying behaviour data from 30 family doctors, each carrying out 4 work-place-related, controlled search tasks. ► Evaluation of impact and retrieval performance of query behaviour. ► Findings supporting and extending previous findings about query exhaustivity. ► Findings indicating the semantic component model as feature to encourage searchers to structure and specify queries. ► Findings indicating that the semantic component model might reduce the vocabulary problem in information retrieval. Professional, workplace searching is different from general searching, because it is typically limited to specific facets and targeted to a single answer. We have developed the semantic component (SC) model, which is a search feature that allows searchers to structure and specify the search to context-specific aspects of the main topic of the documents. We have tested the model in an interactive searching study with family doctors with the purpose to explore doctors’ querying behaviour, how they applied the means for specifying a search, and how these features contributed to the search outcome. In general, the doctors were capable of exploiting system features and search tactics during the searching. Most searchers produced well-structured queries that contained appropriate search facets. When searches failed it was not due to query structure or query length. Failures were mostly caused by the well-known vocabulary problem. The problem was exacerbated by using certain filters as Boolean filters. The best working queries were structured into 2–3 main facets out of 3–5 possible search facets, and expressed with terms reflecting the focal view of the search task. The findings at the same time support and extend previous results about query structure and exhaustivity showing the importance of selecting central search facets and express them from the perspective of search task. The SC model was applied in the highest performing queries except one. The findings suggest that the model might be a helpful feature to structure queries into central, appropriate facets, and in returning highly relevant documents.},
journal = {Information Processing and Management},
pages = {1151--1170},
volume = {48},
publisher = {Elsevier Ltd},
number = {6},
year = {2012},
title = {How doctors search: A study of query behaviour and the impact on search results},
language = {eng},
author = {Lykke, Marianne and Price, Susan and Delcambre, Lois},
keywords = {Query Behaviour ; Work-Place Retrieval ; Family Doctors ; Retrieval Performance},
}

@article{WuYou-Xi2017LIEL,
issn = {10009000},
abstract = {Extreme learning machine (ELM) is a learning algorithm for generalized single-hidden-layer feed-forward networks (SLFNs). In order to obtain a suitable network architecture, Incremental Extreme Learning Machine (I-ELM) is a sort of ELM constructing SLFNs by adding hidden nodes one by one. Although kinds of I-ELM-class algorithms were proposed to improve the convergence rate or to obtain minimal training error, they do not change the construction way of I-ELM or face the over-fitting risk. Making the testing error converge quickly and stably therefore becomes an important issue. In this paper, we proposed a new incremental ELM which is referred to as Length-Changeable Incremental Extreme Learning Machine (LCI-ELM). It allows more than one hidden node to be added to the network and the existing network will be regarded as a whole in output weights tuning. The output weights of newly added hidden nodes are determined using a partial error-minimizing method. We prove that an SLFN constructed using LCI-ELM has approximation capability on a universal compact input set as well as on a finite training set. Experimental results demonstrate that LCI-ELM achieves higher convergence rate as well as lower over-fitting risk than some competitive I-ELM-class algorithms.},
journal = {Journal of Computer Science and Technology},
pages = {630--643},
volume = {32},
publisher = {Springer Science & Business Media},
number = {3},
year = {2017},
title = {Length-Changeable Incremental Extreme Learning Machine},
language = {eng},
address = {Beijing},
author = {Wu, You-Xi and Liu, Dong and Jiang, He},
keywords = {Job Rotation ; Algorithms ; Studies ; Man Machine Interaction ; Machine Learning ; Algorithms ; Convergence ; Feedforward ; Learning (Ci) ; Single-Hidden-Layer Feed-Forward Network (Slfn) ; Incremental Extreme Learning Machine (I-Elm) ; Random Hidden Node ; Convergence Rate ; Universal Approximation},
url = {http://search.proquest.com/docview/1899772774/},
}

@inproceedings{HolmesReid2010Essc,
series = {SUITE '10},
abstract = {<p><p>Developers frequently try to locate references to particular program elements within their systems; however, these queries often return an overwhelming number of results. The result sets for these queries tend to be large because integrated development environments locate matches using static search approaches; however, the developer may be more interested in which references <i>actually</i> happened for a particular execution, instead of which references <i>could</i> happen in a hypothetical execution. We posit that dynamic search approaches can complement customary static search approaches in the same ways dynamic analysis complements static analysis. Specifically, in this paper, we hypothesize that filtering static reference queries with dynamic trace data can reduce the number of results a developer must consider when performing a query, helping them to focus on a subset of the static query results. To test our hypothesis, we filtered the results of the Eclipse <i>find references</i> query with dynamic trace data for three different projects; our preliminary evidence demonstrates that dynamic trace data can be used to effectively filter the result sets of static source code queries.</p></p>},
pages = {13--16},
publisher = {ACM},
booktitle = {Proceedings of 2010 ICSE Workshop on search-driven development: users, infrastructure, tools and evaluation},
isbn = {9781605589626},
year = {2010},
title = {Enhancing static source code search with dynamic data},
language = {eng},
author = {Holmes, Reid and Notkin, David},
}

@article{ManandharDipu2017Lrlf,
issn = {0167-8655},
abstract = {•This paper proposes a new lattice-support repetitive local feature (LS-RLF) detection to perform visual search.•Key contributions include integrated approach for repetitive pattern detection, lattice estimation, and image scoring metric.•Key advantage is that the detected features and image scoring can be used directly in current visual search techniques.•Proposed method achieves an mAP improvement of 4.5% over other state-of-the-art methods (e.g. Torri PAMI 2015). Repetitive patterns such as building facades, floor tiles, vegetation, and wallpapers are commonly found in sceneries and images. The presence of such repetitive patterns in images often leads to visual burstiness and geometric ambiguity, which poses challenge for state-of-the-art visual search technologies. To alleviate these problems, we propose a new lattice-support repetitive local feature detection method to detect repetitive patterns, estimate the underlying lattice structure, and enhance descriptors used for subsequent visual image search. Existing methods for repetitive pattern detection are commonly based on determining the underlying lattice structures. However, these structures do not correspond directly to robust features that are scale- and rotation-invariant. This paper proposes a new lattice-support repetitive local feature (LS-RLF) detection method that aims to integrate lattice information into repeated local feature detection and extraction. The advantage of the proposed method is that the detected features can be directly used by current visual search technologies. The LS-RLF method estimates the undetected repeated features in the lattice structure using Hough transform-based feature estimation. Further, in order to handle the visual burstiness issue, a new LS-RLF based image retrieval framework is developed. Experiments performed on benchmark datasets show that the proposed method outperforms the state-of-the-art methods by mean Average Precisions (mAP) of 4.5%, 5.5% and 3.2% on Oxford, Paris, and INRIA holidays datasets respectively. This demonstrates the effectiveness of the proposed method in performing visual search for images which contain wide range of repeated patterns.},
journal = {Pattern Recognition Letters},
pages = {123--129},
volume = {98},
publisher = {Elsevier B.V.},
year = {2017},
title = {Lattice-Support repetitive local feature detection for visual search},
language = {eng},
author = {Manandhar, Dipu and Yap, Kim-Hui and Miao, Zhenwei and Chau, Lap-Pui},
keywords = {Repetitive Pattern Detection ; Visual Burstiness ; Image Search and Retrieval},
}

@misc{WilliamsAndrew1999Loia,
abstract = {This dissertation addresses learning ontologies in a multiagent system where individual learning agents lack a commitment to a common, pre-defined ontology but share a distributed, collective memory of semantic objects. In this system each agent creates and learn conceptualizations, or ontologies, which can be useful for its individual purposes but will also share its knowledge in order to improve group problem solving performance. My thesis states that multiagent learning of ontologies among individual agents with diverse conceptualizations is feasible and these learned ontologies can be used by the agents to improve group search performance for related semantic concepts through experience in the problem domain. My approach addresses the current weaknesses to sharing knowledge among distributed agents by introducing a theory for learning ontologies based on combining agent communication and machine learning with two novel methodologies: (a) recursive semantic context rule learning and (b) concept cluster integration. I have implemented and used a proof of concept, DOGGIE (Distributed Ontology Gathering Group Integration Environment), to perform my experiments and demonstrate my theory. The results of my evaluation of this novel multiagent learning approach and algorithms contained in this dissertation demonstrate that my thesis is supported.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780599756588},
year = {1999},
title = {Learning ontologies in a multiagent system},
language = {eng},
author = {Williams, Andrew},
keywords = {Electrical Engineering ; Computer Science ; Electrical Engineering ; Computer Science ; Applied Sciences ; Knowledge Sharing ; Machine Learning ; Multiagent System ; Ontologies},
url = {http://search.proquest.com/docview/304530667/},
}

@article{KouFeifei2016Snsb,
issn = {2468-2322},
abstract = {Because of everyone's involvement in social networks, social networks are full of massive multimedia data, and events are got released and disseminated through social networks in the form of multi-modal and multi-attribute heterogeneous data. There have been numerous researches on social network search. Considering the spatio-temporal feature of messages and social relationships among users, we summarized an overall social network search framework from the perspective of semantics based on existing researches. For social network search, the acquisition and representation of spatio-temporal data is the basis, the semantic analysis and modeling of social network cross-media big data is an important component, deep semantic learning of social networks is the key research field, and the indexing and ranking mechanism is the indispensable part. This paper reviews the current studies in these fields, and then main challenges of social network search are given. Finally, we give an outlook to the prospect and further work of social network search.},
journal = {CAAI Transactions on Intelligence Technology},
pages = {293--302},
volume = {1},
publisher = {Elsevier B.V.},
number = {4},
year = {2016},
title = {Social network search based on semantic analysis and learning},
language = {eng},
author = {Kou, Feifei and Du, Junping and He, Yijiang and Ye, Lingfei},
keywords = {Semantic Analysis ; Semantic Learning ; Cross-Modal ; Social Network Search},
}

@article{KhanSaiful2016Opvf,
issn = {1474-0346},
abstract = {•Large scale enterprise search activities can benefit from provenance information.•An ontologies-based search engine can capture semantically-meaningful provenance information.•Provenance visualization can support criteria reformulation and collaborate search activities.•Visualization-assisted enterprise search can provide a cost-effective solution to large scale file search in industry. In many large engineering enterprises, searching for files is a high-volume routine activity. Visualization-assisted search facilities can significantly reduce the cost of such activities. In this paper, we introduce the concept of Search Provenance Graph (SPG), and present a technique for mapping out the search results and externalizing the provenance of a search process. This enables users to be aware of collaborative search activities within a project, and to be able to reason about potential missing files (i.e., false negatives) more effectively. We describe multiple ontologies that enable the computation of SPGs while supporting an enterprise search engine. We demonstrate the novelty and application of this technique through an industrial case study, where a large engineering enterprise needs to make a long-term technological plan for large-scale document search, and has found the visualization-assisted approach to be more cost-effective than alternative approaches being studied.},
journal = {Advanced Engineering Informatics},
pages = {244--257},
volume = {30},
publisher = {Elsevier Ltd},
number = {2},
year = {2016},
title = {Ontology-assisted provenance visualization for supporting enterprise search of engineering and business files},
language = {eng},
author = {Khan, Saiful and Kanturska, Urszula and Waters, Tom and Eaton, James and Bañares-Alcántara, René and Chen, Min},
keywords = {Enterprise Search ; Engineering Document ; Knowledge Management ; Information Visualization ; Provenance Visualization},
}

@misc{Duala-EkokoEkwa2012USRt,
abstract = {Application Programming Interfaces (APIs) allow software developers to reuse code libraries, frameworks, or services without the need of having to implement relevant functionalities from scratch. The benefits of reusing source code or services through APIs have encouraged the adoption of APIs as the building blocks of modern-day software systems. However, leveraging the benefits of APIs require a developer to frequently learn how to use unfamiliar APIs — a process made difficult by the increasing size of APIs, and the increase in the number of APIs with which a developer has to work. In this dissertation, we investigated some of the challenges developers encounter when working with unfamiliar APIs, and we designed and implemented new programming tools to assist developers in learning how to use new APIs. To investigate the difficulties developers encounter when learning to use APIs, we conducted a programming study in which twenty participants completed two programming tasks using real-world APIs. Through a systematic analysis of the screen captured videos and the verbalizations of the participants, we isolated twenty different types of questions the programmers asked when learning to use APIs, and identified five of the twenty questions as the most difficult for the programmers to answer in the context of our study. Drawing from varied sources of evidence, such as the verbalizations and the navigation paths of the participants, we explain why the participants found certain questions hard to answer, and provide new insights to the cause of the difficulties. To facilitate the API learning process, we designed and evaluated two novel programming tools: API Explorer and Introspector. The API Explorer tool addresses the difficulty a developer faces when the API types or methods necessary to implement a task are not accessible from the type the developer is working with. API Explorer leverages the structural relationships between API elements to recommend relevant methods on other objects, and to identify API types relevant to the use of a method or class. The Introspector tool addresses the difficulty of formulating effective queries when searching for code examples relevant to implementing a task. Introspector combines the structural relationships between API types to recommend types that should be used together with a seed to search for code examples for a given task. Using the types recommended by Introspector as search query, a developer can search for code examples across two code repositories, and in return, will get a list of code examples ranked based on their relevance to the search query. We evaluated API Explorer through a programming study, and evaluated Introspector quantitatively using ten tasks from six different APIs. The results of the evaluations suggest that these programming tools provide effective support to programmers learning how to use APIs.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780494790359},
year = {2012},
title = {Using Structural Relationships to Facilitate API Learning},
language = {eng},
author = {Duala-Ekoko, Ekwa},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Application Programming Interfaces},
url = {http://search.proquest.com/docview/1267767696/},
}

@article{RoblesKarina2011Taor,
issn = {0950-5849},
abstract = {Highlights ► Case study shows domain ontology importance in the retrieval process. ► The importance of an element type expansion method by semantic similarity is shown. ► Correlation between query complexity and retrieved elements was identified. ► Positive Return of Investment (ROI) was estimated using Poulin’s Model. ► Semantic technology combined with information retrieval improves retrieval results. Context Software Reuse has always been an important area amongst software companies in order to increase their productivity and the quality of their products, but code reuse is not the only answer for this. Nowadays, reuse techniques proposals include software designs or even software specifications. Therefore, this research focuses on software design, specifically on UML Class Diagrams. A semantic technology has been applied to facilitate the retrieval process for an effective reuse. Objective This research proposes an ontology-based retrieval technique by semantic similarity in order to support effective retrieval process for UML Class Diagrams. Since UML Class Diagrams are a de facto standard in the design stages of a Software Development Process, a good technique is needed to reuse them, i.e. reusing during the design stage instead of just the coding stages. Method An application ontology modeled using UML specifications was designed to compare UML Class Diagram element types. To measure their similarity, a survey was conducted amongst UML experts. Query expansion was improved by a domain ontology supporting the retrieval phase. The calculus of minimal distances in ontologies was solved using a shortest path algorithm. Results The case study shows the domain ontology importance in the UML Class Diagram retrieval process as well as the importance of an element type expansion method, such as an application ontology. A correlation between the query complexity and retrieved elements has been identified, by analyzing results. Finally, a positive Return of Investment (ROI) was estimated using Poulin’s Model. Conclusion Because Software Reuse has not to be limited to the coding stage, approaches to reuse design stage must be developed, i.e. UML Class Diagrams reuse. This approach proposes a technique for UML Class Diagrams retrieval, which is one important step towards reuse. Semantic technology combined with information retrieval improves the retrieval results.},
journal = {Information and Software Technology},
volume = {54},
publisher = {Elsevier B.V.},
number = {1},
year = {2011},
title = {Towards an ontology-based retrieval of UML Class Diagrams},
language = {eng},
author = {Robles, Karina and Fraga, Anabel and Morato, Jorge and Llorens, Juan},
keywords = {Information Retrieval ; Ontologies ; Software Reuse ; Software Engineering ; Uml Class Diagrams},
}

@inproceedings{RaghothamanMukund2016Sswi,
series = {ICSE '16},
abstract = {<p><p>Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as "generate md5 hash code". The query does not need to contain framework-specific trivia such as the type names or methods of interest.</p> <p>We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce <i>structured call sequences</i> to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis.</p> <p>We evaluated swim with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.</p></p>},
pages = {357--367},
volume = {14-22-},
publisher = {ACM},
booktitle = {Proceedings of the 38th International Conference on software engineering},
isbn = {9781450339001},
year = {2016},
title = {SWIM: synthesizing what i mean: code search and idiomatic snippet synthesis},
language = {eng},
author = {Raghothaman, Mukund and Wei, Yi and Hamadi, Youssef},
keywords = {Natural Languages ; Pattern Matching ; Search Engines ; C# Languages ; Data Models ; Libraries ; Web Pages ; Free Form Queries ; Code Search ; Idiomatic Snippet Synthesis ; Structured Call Sequences ; Computing and Processing;},
}

@inproceedings{YuHaibo2016Aaea,
series = {Internetware '16},
abstract = {<p><p>Software libraries have become more and more complex in recent years. Developers usually have to rely on search engines to find API documents and then select suitable APIs to do relevant development when working on unfamiliar functions. However, the traditional search engines do not focus on searching APIs that make this process inconvenient and time consuming. Although a lot of efforts have been made on API understanding and code search in industry and academia, work and tools that can recommend API methods to users based on their description of API's functionality are still very limited.</p> <p>In this paper, we propose a search-based recommendation algorithm on API methods. We call the algorithm APIBook and implement an API method recommendation tool based on the proposed algorithm. The algorithm can recommend relevant API methods to users based on user input written in natural language. This algorithm combines semantic relevance, type relevance and the extent of degree that API method is used to sort these API methods and rank those that are highly relevant and widely used in the top positions. Examples of codes in real projects are also provided to help users to learn and to understand the API method recommended. The API recommendation tool selects the Java Standard Library as well as 100 popular open source libraries as API recommending material. Users can input the API description via the Web interface, and view the search results with sample codes on screen.</p> <p>The evaluation experiment is performed and the result shows that APIBook is more effective for finding APIs than traditional search models and it takes on average 0.7 seconds for finding relevant API methods which we think to be reasonable for satisfying daily query requirements.</p></p>},
pages = {45--53},
volume = {18-},
publisher = {ACM},
booktitle = {Proceedings of the 8th Asia-Pacific Symposium on internetware},
isbn = {9781450348294},
year = {2016},
title = {APIBook: an effective approach for finding APIs},
language = {eng},
author = {Yu, Haibo and Song, Wenhao and Mine, Tsunenori},
keywords = {Api Recommendation ; Api Search ; Information Retrieval ; Program Analysis},
}

@misc{ZhuDongqing2014Irfr,
abstract = {Medical professionals leverage health-related data to address questions and support decision-makings. However, many of these medical tasks require intensive manual effort in identifying useful information in the noisy data. The rapid growth of data is making these tasks more and more costly and time-consuming. In this thesis, we develop effective medical information retrieval (IR) systems to reduce search-related manual work for three representative medical related tasks, namely electronic medical records (EMR) based cohort identification, Medical Subject Headings (MeSH) indexing, and gene ontology annotation (GOA). For cohort identification, we improve the search precision and recall from three aspects: 1) we design a multi-level evidence aggregation strategy for effective merging and scoring of the distributed evidence in EMR; 2) we develop a novel statistical IR model that significantly alleviates two medical language related issues in medical IR; 3) we further enhance the search performance by effectively incorporating domain knowledge into our system. For MeSH indexing and GOA, we demonstrate how to use IR to address specific needs. In particular, we investigate different query formulation methods and explore various ways in which IR work together with other techniques such as Natural Language Processing and Machine Learning.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781321607819},
year = {2014},
title = {Information retrieval for reducing manual effort in biomedical and clinical research},
language = {eng},
author = {Zhu, Dongqing},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Information Retrieval ; Medical Record Search ; Retrieval Models},
url = {http://search.proquest.com/docview/1661456759/},
}

@inproceedings{2015AiAI,
series = {Lecture Notes in Computer Science},
volume = {9413},
publisher = {Springer International Publishing},
booktitle = {Advances in Artificial Intelligence and Soft Computing},
isbn = {978-3-319-27059-3},
year = {2015},
title = {Advances in Artificial Intelligence and Soft Computing: 14th Mexican International Conference on Artificial Intelligence, MICAI 2015, Cuernavaca, Morelos, Mexico, October 25–31, 2015, Proceedings, Part I},
language = {eng},
address = {Cham},
keywords = {Computer Science -- Artificial Intelligence (incl. Robotics); Computer Science -- Computer Imaging, Vision, Pattern Recognition and Graphics; Computer Science -- Health Informatics; Computer Science -- Information Systems Applications (incl. Internet); Computer Science -- Information Storage and Retrieval; Computer Science -- Algorithm Analysis and Problem Complexity},
}

@misc{MartieLee2017UtIo,
abstract = {Sometimes, when programmers use a search engine, they know more or less what they need. Other times, programmers use the search engine to look around and generate possible ideas for the programming problem on which they are working. The key insight we explore in this dissertation is that, in the latter case, the results found tend to serve as inspiration or triggers for the next queries issued. We introduce two search engines, CodeExchange and CodeLikeThis, both of which are specifically designed to enable the user to directly leverage results from a previous query in formulating a next query. CodeExchange does this with a set of four features that enable the programmer to use characteristics of the results to find other code with or without those characteristics. For instance, by selecting characteristics of the results the programmer likes (e.g., libraries used or method calls) or dislikes (e.g., code complexity or size), the programmer can refine their query for results with or without those characteristics. CodeLikeThis explores a different mechanism of supporting iteration by letting developers simply select an entire result to find code that is analogous, to some degree, to that result. For instance, the developer can select an algorithm implementation (e.g., quick sort) with a directive to find more similar implementations, less similar implementations (e.g., heap sort), or somewhat similar implementations. We evaluated the impact of CodeExchange and CodeLikeThis on the experience, time, and success of the code search process. We compared our iterative approaches with two approaches not explicitly supporting iteration, a baseline and Google, in a laboratory experiment among 24 developers. We found that search engines that support using results to form the next query can improve the programmers' search experience and different approaches to iteration can provide better experiences depending on the kind of task. The main contributions of this dissertation are six-fold. First, it contributes a new approach to code search, implemented in CodeExchange, that supports the programmer in iteratively searching by bringing characteristics of the results into their query. Second, it contributes, a new approach to code search, implemented in CodeLikeThis, that supports the programmer in iteratively searching by simply selecting a result to issue a query for other similar code. Third, it contributes an extensive laboratory experiment evaluating the impact of iterative approaches on the experience, time, and success of the code search process. Fourth, it contributes new findings about how developers search for code . Fifth, it contributes the implementation of CodeExchange and CodeLikeThis as fully functioning search engines over 10 million Java classes mined off the Internet. Sixth, it contributes an Index of 10 million Java classes indexed by different technical and social properties.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780355310894},
year = {2017},
title = {Understanding the Impact of Support for Iteration on Code Search},
language = {eng},
author = {Martie, Lee},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Code Search ; Internet-Scale ; Iteartive ; Programming},
url = {http://search.proquest.com/docview/1983460354/},
}

@misc{GallardoValenciaRosalva2012HSDS,
abstract = {The large amount of information available on the Web has changed the way people develop software. Even though looking for source code on the Web is a common practice among developers, little is known about what motivates developers to look for source code on the Web, how developers evaluate search results, how they use the information they find, and how efficient are these Web searches in helping them complete software tasks. We found that looking for source code on the Web is a common activity for software developers because it helps them solve software development problems efficiently. Developers evaluate search results by making quick judgments and examining options in a serial fashion until a good-enough candidate is found. Information found on the Web is used to build developers' knowledge or to guide their coding. Using a series of empirical studies including online questionnaires, focus groups, laboratory experiments, and field studies in the US and abroad, we gained a better understanding of how software developers solve problems by searching for source code on the Web. We found that 83% of developers performed at least one Web search during a work day and on average they did 3.6 searches per day. We also found that 82% of Web searches are done to solve opportunistic problems, such as when developers need to remember syntax details, to clarify implementation details or fix bugs, and to learn new concepts. These searches are not planned ahead of time; they are done as they are needed. Using a naturalistic decision making approach, we found that developers make rapid judgments to evaluate search results in a serial fashion to find a good-enough candidate to solve their opportunistic software problems. We also found that developers are able to successfully solve 63% of their opportunistic software problems in 4.9 minutes on average by using the information they found on the Web to build their knowledge or to guide their coding. Results from these empirical studies have implications for tool designers, researchers, and developers.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781267240187},
year = {2012},
title = {How Software Developers Solve Problems by Searching for Source Code on the Web: Studies on Judgments in Evaluation of Results and Information Use},
language = {eng},
author = {Gallardo Valencia, Rosalva},
keywords = {Information Technology ; Web Studies ; Computer Science ; Information Technology ; Web Studies ; Computer Science ; Communication and the Arts ; Applied Sciences ; Empirical Studies ; Evaluation of Search Results ; Internet-Scale Code Search ; Naturalistic Decision Making ; Source Code Search on the Web ; Source Codes},
url = {http://search.proquest.com/docview/943455111/},
}

@inproceedings{WeiXing2009Cssd,
series = {CIKM '09},
abstract = {<p><p>We propose a simple yet effective approach to context sensitive synonym discovery for Web search queries based on co-click analysis; i.e., analyzing queries leading to clicking same documents. In addition to deriving word based synonyms, we also derive concept based synonyms with the help of query segmentation. Evaluation results show that this approach dramatically outperforms the thesaurus based synonym replacement method in keeping search intent, from accuracy of 40% to above 80%.</p></p>},
pages = {1585--1588},
publisher = {ACM},
booktitle = {Proceedings of the 18th ACM conference on information and knowledge management},
isbn = {9781605585123},
year = {2009},
title = {Context sensitive synonym discovery for web search queries},
language = {eng},
author = {Wei, Xing and Peng, Fuchun and Tseng, Huihsin and Lu, Yumao and Dumoulin, Benoit},
keywords = {Query Reformulation ; Synonym Discovery},
}

@misc{AgarwalaMegha2010Dsde,
abstract = {This thesis describes the investigation conducted to discover if software developers exhibit cognitive biases while searching for source code on the Web. A cognitive bias is a person's tendency to make errors in judgment based on his processing of information and application of knowledge. We attempted to gain a deeper understanding of the search process followed by developers by examining if they suffered from cognitive biases. Biases investigated are order bias, conjunction bias and exposure bias. This investigation is a re-analysis of data from a laboratory experiment where twenty four participants were each given a search scenario and asked to perform the search using five different code search engines. They were allowed to use the engines in any order and they had to judge the relevance of first ten matches returned. The dependent variables in our analysis were time spent on the search engine, number of queries entered in a search engine, number of terms in a query, percentage of the matches clicked and precision of the first ten matches returned. The independent variables in this analysis were order in which an engine was used, motivation and size of the search target (came from the scenario assigned to the subjects), frequency of searching source code and number of years of experience (which came from the background questionnaire filled by the subjects). Background information together with the scenario was studied to investigate for variation in performance of the subjects. After the analysis, we found evidence of order bias, but not for conjunction and exposure bias. We found that the subjects spent most of their time on earlier search engines and spent less time on subsequent search engines, (F (4, 92) =11.36, P<0.0001). The subjects had a significant difference in their click-through percentage by order of use of the engine, (F (4, 92) =2.89, P<0.05). In many statistical tests, there was a subject effect, but further analysis revealed this to be a scenario effect. In other words, motivation and size of the search target were found to influence the different steps of the search process.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781109661415},
year = {2010},
title = {Do software developers experience cognitive bias while searching for source code on the Web?},
language = {eng},
author = {Agarwala, Megha},
keywords = {Web Studies ; Information Science ; Computer Science ; Web Studies ; Information Science ; Computer Science ; Communication and the Arts ; Applied Sciences},
url = {http://search.proquest.com/docview/305188352/},
}

@article{HuangMutao2011USaR,
issn = {1364-8152},
abstract = {The main barriers to make full use of the wealth of available online data is that users are unable to rapidly locate relevant web services and retrieve appropriate data sets from different data repositories as well as efficiently reconcile integration between temporal and geospatial data. To address these issues, this paper focuses on the development of an online Water Data Discovery and Retrieval system (WDDRs) to enhance the capabilities of services discovery, data retrieval, and data visualization. The most significant features of WDDRs prototype are reflected in two aspects. On one hand, a water ontology incorporated with Universal Description, Discovery and Integration (UDDI) based enhanced services catalog offers facilities to alleviate semantic heterogeneity and associate semantic information with the web services discovery and data retrieval process. On the other hand, by embracing the capability within the context of Service Oriented Architectures (SOA) and leveraging the latest protocols of several open web service standards and two popular RIAs (Rich Internet Applications) frameworks including the Microsoft Silverlight and the ESRI ArcGIS API for Silverlight, this system provides an interactive web portal which enables users to one-stop search, access, download and visualize different types of geospatial and observational data in a single environment. With the aim of supporting the study of integrated water environmental assessment, several investigations about water data discovery and retrieval were implemented to demonstrate the feasibility and effectiveness of the WDDRs. Highlights ► We propose a mapping mechanism that allows for interoperation between water ontology and UDDI. ► We combine existing standards to enable on-the fly services discovery and one-stop data retrieval. ► We present a novel way of reconciling integration between geospatial data and temporal data.},
journal = {Environmental Modelling and Software},
pages = {1309--1324},
volume = {26},
publisher = {Elsevier Ltd},
number = {11},
year = {2011},
title = {Using SOA and RIAs for water data discovery and retrieval},
language = {eng},
author = {Huang, Mutao and Maidment, David R. and Tian, Yong},
keywords = {Ontology ; Services Discovery ; Data Retrieval ; Data Visualization ; Web Portal ; Service Oriented Architectures ; Rich Internet Applications},
}

@incollection{2010ODDI,
abstract = {The concept of Semantic Web (Berners, 2001) introduces a new form of knowledge representation – an ontology. An ontology is a partially ordered set of words and concepts of a specific domain, and allows for defining different kinds of relationships existing among concepts. Such approach promises formation of an environment where information is easily accessible and understandable for any system, application and/or human. Hierarchy of concepts (Yager, 2000) is a different and very interesting form of knowledge representation. A graph-like structure of the hierarchy provides a user with a suitable tool for identifying variety of different associations among concepts. These associations express user’s perceptions of relations among concepts, and lead to representing definitions of concepts in a human-like way. The Internet becomes an overwhelming repository of documents. This enormous storage of information will be effectively used when users will be equipped with systems capable of finding related documents quickly and correctly. The proposed work addresses that issue. It offers an approach that combines a hierarchy of concepts and ontology for the task of identifying web documents in the environment of the Semantic Web. A user provides a simple query in the form a hierarchy that only partially “describes” documents (s)he wants to retrieve from the web. The hierarchy is treated as a “seed” representing user’s initial knowledge about concepts covered by required documents. Ontologies are treated as supplementary knowledge bases. They are used to instantiate the hierarchy with concrete information, as well as to enhance it with new concepts initially unknown to the user. The proposed approach is used to design a prototype system for document identification in the web environment. The description of the system and the results of preliminary experiments are presented.},
pages = {186--220},
booktitle = {Progressive Concepts for Semantic Web Evolution},
isbn = {9781605669922},
year = {2010},
title = {Ontology Driven Document Identification in Semantic Web},
}

@inproceedings{HoffmannRaphael2007Afal,
series = {UIST '07},
abstract = {<p><p>Programmers regularly use search as part of the development process, attempting to identify an appropriate API for a problem, seeking more information about an API, and seeking samples that show how to use an API. However, neither general-purpose search engines nor existing code search engines currently fit their needs, in large part because the information programmers need is distributed across many pages. We present Assieme, a Web search interface that effectively supports common programming search tasks by combining information from Web-accessible Java Archive (JAR) files, API documentation, and pages that include explanatory text and sample code. Assieme uses a novel approach to finding and resolving implicit references to Java packages, types, and members within sample code on the Web. In a study of programmers performing searches related to common programming tasks, we show that programmers obtain better solutions, using fewer queries, in the same amount of time spent using a general Web search interface.</p></p>},
pages = {13--22},
publisher = {ACM},
booktitle = {Proceedings of the 20th annual ACM symposium on user interface software and technology},
isbn = {9781595936790},
year = {2007},
title = {Assieme: finding and leveraging implicit references in a web search interface for programmers},
language = {eng},
author = {Hoffmann, Raphael and Fogarty, James and Weld, Daniel},
keywords = {Implicit References ; Web Search Interfaces ; Computer Science},
}

@misc{EtterDavid2015MRLf,
abstract = {Known Item Search (KIS) is a specialized task of the general multimedia search problem. KIS describes the scenario where a user has seen a video before, must formulate a text description based on what he remembers, and knows that there is only one correct answer. The KIS task takes as input a text-only description and returns the ranked list of videos most likely to match the known item. A KIS query is a verbose text description which is used to search a video repository consisting of metadata, audio, and visual content. The task presents a challenge in mapping the unique views of the video and query into a common feature space for search and ranking. Additionally, the queries often include key terms or phrases which are mapped into an incorrect multimedia view. The mapping problem causes the result set to drift away from the intended meaning of the original query. Supervised learning approaches to the KIS problem must overcome the imbalance of positive to negative examples that results from having a single known item. We introduce a multiview rank learning approach to KIS, based on boosted regression trees, which provides a common feature space and overcomes the view ranking challenge. Natural language processing techniques are used to address the view drift problem by extracting key phrases from the original query which align with a specific video view. This approach allows us to activate only those views of the video which are applicable to the given query. A semi-supervised rank learning approach is used to overcome the class imbalance of having a single known item. Pseudo-positive examples are identified in a similarity graph and a K-Step Markov approach is used to estimate the importance of nodes relative to the truth root node. We evaluate our approach using benchmark datasets from the TRECVid evaluation and a large social media collection.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781321811346},
year = {2015},
title = {Multiview Rank Learning for Multimedia Known Item Search},
language = {eng},
author = {Etter, David},
keywords = {Multimedia Communications ; Computer Science ; Multimedia Communications ; Computer Science ; Communication and the Arts ; Applied Sciences ; Known Item Search ; Multimedia ; Multiview ; Ranking},
url = {http://search.proquest.com/docview/1695806471/},
}

@article{YamashitaTakanori2015VoKF,
issn = {1877-0509},
abstract = {The secondary use of medical data to improve medical care is gaining much attention. We have analyzed electronic clinical pathways for improving the medical process. The analysis of clinical pathways so far has used statistics analysis models, however as issue remains that the order, and multistory spatial and time relations of the each factor could not be analyzed. We constructed an Outcome tree system that shows the greatest significant relation for each factor. The Hip replacement arthroplasty clinical pathway was analyzed by the system, and the outcome variance of the clinical pathway was visualized. The results indicate the path of patient's who have a long hospitalization stay and extracted four critical indicators.},
journal = {Procedia Computer Science},
pages = {342--351},
volume = {60},
publisher = {Elsevier B.V.},
year = {2015},
title = {Visualization of Key Factor Relation in Clinical Pathway},
language = {eng},
author = {Yamashita, Takanori and Flanagan, Brendan and Wakata, Yoshifumi and Hamai, Satoshi and Nakashima, Yasuharu and Iwamoto, Yukihide and Nakashima, Naoki and Hirokawa, Sachio},
keywords = {Outcome Tree ; Spanning Tree ; Clinical Pathway ; Critical Indicator ; Hip Replacement Arthroplasty},
}

@article{HuangYo-Ping2009Arkp,
issn = {0957-4174},
abstract = {This paper presents a knowledge-based plant information retrieval system that is robust to inaccurate and erroneous user queries. First, a knowledge-based genetic algorithm (GA) corrects the erroneous input vectors before these are fed into a back-propagation neural network (BPNN) that performs the actual query. Experimental results show that the strategy achieves a 75% recall rate and 25% precision rate with a cutoff level of 10 under the misjudgment of shapes. Moreover, a fully trained BPNN dynamically adapts to changes in the environment. Due to its robust and simple user interface and portability, the strategy is particularly applicable to educational settings such as outdoor fieldwork in courses on ecology.},
journal = {Expert Systems With Applications},
pages = {675--682},
volume = {36},
publisher = {Elsevier Ltd},
number = {1},
year = {2009},
title = {A robust knowledge-based plant searching strategy},
language = {eng},
author = {Huang, Yo-Ping and Tsai, Tienwei and Wu, Yan-Ming and Sandnes, Frode-Eika},
keywords = {Information Retrieval ; Knowledge-Based Model ; Genetic Algorithm ; Back-Propagation Neural Network},
}

@article{WildemuthBarbara2004Teod,
issn = {15322882},
abstract = {A search tactic is a set of search moves that are temporally and semantically related. The current study examined the tactics of medical students searching a factual database in microbiology. The students answered problems and searched the database on three occasions over a 9-month period. Their search moves were analyzed in terms of the changes in search terms used from one cycle to the next, using two different analysis methods. Common patterns were found in the students' search tactics; the most common approach was the specification of a concept, followed by the addition of one or more concepts, gradually narrowing the retrieved set before it was displayed. It was also found that the search tactics changed over time as the students' domain knowledge changed. These results have important implications for designers in developing systems that will support users' preferred ways of formulating searches. In addition, the research methods used (the coding scheme and the two data analysis methods - zero-order state transition matrices and maximal repeating patterns [MRP] analysis) are discussed in terms of their validity in future studies of search tactics. [PUBLICATION ABSTRACT]},
journal = {Journal of the American Society for Information Science and Technology},
pages = {246--258},
volume = {55},
publisher = {Wiley Periodicals Inc.},
number = {3},
year = {2004},
title = {The effects of domain knowledge on search tactic formulation},
language = {eng},
address = {Hoboken},
author = {Wildemuth, Barbara},
keywords = {Studies ; Information Retrieval ; Searches ; Information Technology ; End Users ; Students ; Experimental/Theoretical ; Information Technology Management},
url = {http://search.proquest.com/docview/231363176/},
}

@article{GärtnerMarkus2014Bsau,
issn = {0219-1377},
abstract = {In this paper, we identify the problems of current semantic and hybrid search systems, which seek to bridge structure and unstructured data, and propose solutions. We introduce a novel input mechanism for hybrid semantic search that combines the clean and concise input mechanisms of keyword-based search engines with the expressiveness of the input mechanisms provided by semantic search engines. This interactive input mechanism can be used to formulate ontology-aware search queries without prior knowledge of the ontology. Furthermore, we propose a system architecture for automatically fetching relevant unstructured data, complementing structured data stored in a Knowledge Base, to create a combined index. This combined index can be used to conduct hybrid semantic searches which leverage information from structured and unstructured sources. We present the reference implementation Hybrid Semantic Search System ( $$HS^3$$ H S 3 ), which uses the combined index to put hybrid semantic search into practice and implements the interactive ontology-enhanced keyword-based input mechanism. For demonstration purpose, we apply $$HS^3$$ H S 3 to the tourism domain. We present performance test results and the results of a user evaluation. Finally, we provide instructions on how to apply $$HS^3$$ H S 3 to arbitrary domains.},
journal = {Knowledge and Information Systems},
pages = {761--792},
volume = {41},
publisher = {Springer London},
number = {3},
year = {2014},
title = {Bridging structured and unstructured data via hybrid semantic search and interactive ontology-enhanced query formulation},
language = {eng},
address = {London},
author = {Gärtner, Markus and Rauber, Andreas and Berger, Helmut},
keywords = {Semantic Web ; User interface design ; Semantic annotation ; Hybrid Semantic Search System ; Keyword-based search ; Concept-based search ; Ontologies},
}

@misc{HuangShu2013Muco,
abstract = {In solving diverse data management problems, underlying social network between users and semantics hidden deep in User-generated Contents (UGC) can be useful from many perspectives. Finding and applying such hidden semantics of UGC and social correlations illustrates a new way in solving various problems. In this thesis, we study several challenging data management problems to investigate how to apply the framework of UGC mining and social network analysis to substantially improve existing solutions. In particular, we focus on the following four problems: First, we propose a novel query expansion technique in Information Retrieval that exploits the “location-based” correlation between users and search engine user logs. We explore the vocabulary of users from different geographic locations and investigate the semantic relations among the documents they search for. Based on that, a hierarchical location and topic based query expansion model is proposed to improve the accuracy of web search. Our proposed model predicts the query location sensitivity with more than 80% precision. Using the model, the final search result is significantly better than several existing query expansion methods. Second, we explore the aggregate social activity and evaluate the significance of various activity features in determining the social activity evolution. In particular, we look in to various formats of social activities and measure how member activity impacts the evolution of the active population. Several activity features are extracted and their impact on the community evolution is evaluated with a feature selection model. Based on the model, the most significant features are identified. Third, we study UGC on Twitter, a large online platform of social media, to identify tweet topics and sentiments towards some preset brands/products. To help understand brand perception and customer opinions, we utilize the correlation of tweet sentiments and topics, and propose a multi-task multi-label (MTML) classification model that performs classification of both sentiments and topics simultaneously. It incorporates results of each task from prior steps to promote and reinforce the other iteratively. Meanwhile, by using multiple labels, the class ambiguity can be addressed. Compared with baselines, MTML produces a much higher accuracy of both sentiment and topic classification. Furthermore, based on tweet sentiment analysis, social network among Twitter users is also taken into consideration to investigate the impact of events on tweet sentiment change. By mining tweets about 2012 USA presidential campaign, we analyze the sentiments towards the presidential candidates. Meanwhile, we incorporate social correlation between Twitter users and present a method to predict the impact of events based on social activities. Analysis on tweets collected over 8 months shows that our method can predict the sentiment change with high accuracy. Mining UGC and social network is not only efficient but also effective in predicting the impact of events.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781339127736},
year = {2013},
title = {Mining user-generated contents on the web and social networks},
language = {eng},
author = {Huang, Shu},
keywords = {Information Science ; Computer Science ; Information Science ; Computer Science ; Communication and the Arts ; Applied Sciences ; Data Mining ; Information Retrieval ; Social Network ; User-Generated Contents},
url = {http://search.proquest.com/docview/1724678486/},
}

@article{ChochlovMuslim2017Ahta,
issn = {0950-5849},
abstract = {ContextFeature location is the task of finding the source code that implements specific functionality in software systems. A common approach is to leverage textual information in source code against a query, using Information Retrieval (IR) techniques. To address the paucity of meaningful terms in source code, alternative, relevant source-code descriptions, like change-sets could be leveraged for these IR techniques. However, the extent to which these descriptions are useful has not been thoroughly studied. ObjectiveThis work rigorously characterizes the efficacy of source-code lexical annotation by change-sets (ACIR), in terms of its best-performing configuration. MethodA tool, implementing ACIR, was used to study different configurations of the approach and to compare them to a baseline approach (thus allowing comparison against other techniques going forward). This large-scale evaluation employs eight subject systems and 600 features. ResultsIt was found that, for ACIR: (1) method level granularity demands less search effort; (2) using more recent change-sets improves effectiveness; (3) aggregation of recent change-sets by change request, decreases effectiveness; (4) naive, text-classification-based filtering of “management” change-sets also decreases the effectiveness. In addition, a strongly pronounced dichotomy of subject systems emerged, where one set recorded better feature location using ACIR and the other recorded better feature location using the baseline approach. Finally, merging ACIR and the baseline approach significantly improved performance over both standalone approaches for all systems. ConclusionThe most fundamental finding is the importance of rigorously characterizing proposed feature location techniques, to identify their optimal configurations. The results also suggest it is important to characterize the software systems under study when selecting the appropriate feature location technique. In the past, configuration of the techniques and characterization of subject systems have not been considered first-class entities in research papers, whereas the results presented here suggests these factors can have a big impact.},
journal = {Information and Software Technology},
pages = {110--126},
volume = {88},
publisher = {Elsevier B.V.},
year = {2017},
title = {A historical, textual analysis approach to feature location},
language = {eng},
author = {Chochlov, Muslim and English, Michael and Buckley, Jim},
keywords = {Feature Location ; Version Histories ; Dataset Expansion ; Software Systems’ Characterization ; Search Effort},
}

@article{NguyenVinh-Tiep2017Padf,
issn = {0350-5596},
journal = {Informatica},
volume = {41},
publisher = {Slovenian Society Informatika},
number = {2},
year = {2017},
title = {Persons-in-places: a deep features based approach for searching a specific person in a specific location.(Report)},
language = {eng},
author = {Nguyen, Vinh-Tiep and Ngo, Thanh Due and Tran, Minh-Triet and Le, Duy-Dinh and Duong, Due Anh},
keywords = {Computer Vision – Research ; Video Recordings – Technology Application},
}

@article{2015DLPQ,
series = {Lecture Notes in Computer Science},
volume = {9469},
publisher = {Springer International Publishing},
booktitle = {Digital Libraries: Providing Quality Information},
isbn = {978-3-319-27973-2},
year = {2015},
title = {Digital Libraries: Providing Quality Information: 17th International Conference on Asia-Pacific Digital Libraries, ICADL 2015, Seoul, Korea, December 9–12, 2015, Proceedings},
language = {eng},
address = {Cham},
keywords = {Computer Science -- Information Storage and Retrieval; Computer Science -- Document Preparation and Text Processing; Computer Science -- Database Management; Computer Science -- Information Systems Applications (incl. Internet)},
}

@article{BorgMarkus2014Rfad,
issn = {1382-3256},
abstract = {Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based trace recovery.},
journal = {Empirical Software Engineering},
pages = {1565--1616},
volume = {19},
publisher = {Springer US},
number = {6},
year = {2014},
title = {Recovering from a decade: a systematic mapping of information retrieval approaches to software traceability},
language = {eng},
address = {Boston},
author = {Borg, Markus and Runeson, Per and Ardö, Anders},
keywords = {Traceability ; Information retrieval ; Software artifacts ; Systematic mapping study},
}

@inproceedings{2017DSTI,
series = {Communications in Computer and Information Science},
volume = {728},
publisher = {Springer Singapore},
booktitle = {Data Science},
isbn = {978-981-10-6387-9},
year = {2017},
title = {Data Science: Third International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2017, Changsha, China, September 22–24, 2017, Proceedings, Part II},
language = {eng},
address = {Singapore},
keywords = {Computer Science -- Data Mining and Knowledge Discovery; Computer Science -- Artificial Intelligence (incl. Robotics); Computer Science -- Image Processing and Computer Vision; Computer Science -- Pattern Recognition},
}

@article{StoleeKathryn2014StSf,
issn = {1557-7392},
abstract = {<p><p>Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries.</p> <p>We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification.</p> <p>We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find <i>approximate</i> matches and then guide modifications to match the user specifications when <i>exact</i> matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified.</p></p>},
journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
pages = {1--45},
volume = {23},
publisher = {ACM},
number = {3},
year = {2014},
title = {Solving the Search for Source Code},
language = {eng},
author = {Stolee, Kathryn and Elbaum, Sebastian and Dobos, Daniel},
keywords = {Smt Solvers ; Semantic Code Search ; Lightweight Specification ; Symbolic Analysis ; Computer Science},
}

@article{FersiniE2012Ssoj,
issn = {0033-0337},
abstract = {<p> Purpose - The need of tools for content analysis, information extraction and retrieval of multimedia objects in their native form is strongly emphasized into the judicial domain: digital videos represent a fundamental informative source of events occurring during judicial proceedings that should be stored, organized and retrieved in short time and with low cost. This paper seeks to address these issues.Design methodology approach - In this context the JUMAS system, stem from the homonymous European Project (www.jumasproject.eu), takes up the challenge of exploiting semantics and machine learning techniques towards a better usability of multimedia judicial folders.Findings - In this paper one of the most challenging issues addressed by the JUMAS project is described: extracting meaningful abstracts of given judicial debates in order to efficiently access salient contents. In particular, the authors present an ontology enhanced multimedia summarization environment able to derive a synthetic representation of judicial media contents by a limited loss of meaningful information while overcoming the information overload problem.Originality value - The adoption of ontology-based query expansion has made it possible to improve the performance of multimedia summarization algorithms with respect to the traditional approaches based on statistics. The effectiveness of the proposed approach has been evaluated on real media contents, highlighting a good potential for extracting key events in the challenging area of judicial proceedings.</p>},
journal = {Program},
pages = {199--219},
volume = {46},
publisher = {Emerald Group Publishing Limited},
number = {2},
year = {2012},
title = {Semantic storyboard of judicial debates: a novel multimedia summarization environment},
language = {eng},
author = {Fersini, E and Sartori, F},
keywords = {Multimedia Summarization ; Ontological Query Expansion ; Multimedia ; Justice ; Information Retrieval ; Library & Information Science},
}

@misc{ZhangZhiwei2017BSRB,
abstract = {Information retrieval (IR) is an important research area that studies how to find the most useful information to satisfy users' needs. Typical IR applications include search engine, e-commerce, social network, etc. Among various evaluation criteria, relevance is often the most important one, i.e. the retrieved information should accurately match the users' intentions. However, in many real world scenarios, relevance is not the only factor under consideration. For example, a recommender system may wish to generate accurate recommendations within affordable time limits; or, a search engine may wish to reduce redundant web pages present to users, even if all of them are relevant. In these cases, an extra criterion (e.g. efficiency, diversity) has to be considered beyond relevance. How to make such a good balance between the traditional relevance criterion and the other criteria becomes an important research task. In this dissertation, we study how to balance heterogeneous criteria in real world IR applications. By the term “heterogeneous”, we mean criteria that are vastly different from relevance, which implies that they cannot be perfectly satisfied by merely optimizing relevance. We first cast the problem into a multi-objective learning framework, and discuss how multi-objective learning can be mathematically formulated. Then we present four case studies, which cover two IR scenarios (web search and recommendation) and three distinct criteria beyond relevance (efficiency, intrinsic diversity and profit). Specifically, the four cases are: efficiency in recommendation, intrinsic diversity in web search, efficiency in web search and profit in recommendation. We elaborate how a multi-objective learning problem is set up to suit each case, and present experiments to show the effectiveness of our proposed algorithms.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780355262230},
year = {2017},
title = {Beyond Simple Relevance: Balancing Heterogeneous Criteria in Information Retrieval Applications},
language = {eng},
author = {Zhang, Zhiwei},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Information Retrieval ; Multi-Objective Learning ; Recommder System ; Web Search},
url = {http://search.proquest.com/docview/1959307648/},
}

@misc{McmillanCollin2012SSaS,
abstract = {As programmers develop software, they instinctively sense that source code exists that could be reused if found — many programming tasks are common to many software projects across different domains. Oftentimes, a programmer will attempt to create new software from this existing source code, such as third-party libraries or code from online repositories. Unfortunately, several major challenges make it difficult to locate the relevant source code and to reuse it. First, there is a fundamental mismatch between the high-level intent reflected in the descriptions of source code, and the low-level implementation details. This mismatch is known as the concept assignment problem , and refers to the frequent case when the keywords from comments or identifiers in code do not match the features implemented in the code. Second, even if relevant source code is found, programmers must invest significant intellectual effort into understanding how to reuse the different functions, classes, or other components present in the source code. These components may be specific to a particular application, and difficult to reuse. One key source of information that programmers use to understand source code is the set of relationships among the source code components. These relationships are typically structural data, such as function calls or class instantiations. This structural data has been repeatedly suggested as an alternative to textual analysis for search and reuse, however as yet no comprehensive strategy exists for locating relevant and reusable source code. In my research program, I harness this structural data in a unified approach to creating and evolving software from existing components. For locating relevant source code, I present a search engine for finding applications based on the underlying Application Programming Interface (API) calls, and a technique for finding chains of relevant function invocations from repositories of millions of lines of code. Next, for reusing source code, I introduce a system to facilitate building software prototypes from existing packages, and an approach to detecting similar software applications.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781267781956},
year = {2012},
title = {Searching, Selecting, and Synthesizing Source Code Components},
language = {eng},
author = {Mcmillan, Collin},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Class Instantiations ; Function Calls ; Keywords ; Mismatch ; Source Codes},
url = {http://search.proquest.com/docview/1221525995/},
}

@article{YangXin-Li2016WSQD,
issn = {10009000},
abstract = {Security has always been a popular and critical topic. With the rapid development of information technology, it is always attracting people's attention. However, since security has a long history, it covers a wide range of topics which change a lot, from classic cryptography to recently popular mobile security. There is a need to investigate security-related topics and trends, which can be a guide for security researchers, security educators and security practitioners. To address the above-mentioned need, in this paper, we conduct a large-scale study on security-related questions on Stack Overflow. Stack Overflow is a popular on-line question and answer site for software developers to communicate, collaborate, and share information with one another. There are many different topics among the numerous questions posted on Stack Overflow and security-related questions occupy a large proportion and have an important and significant position. We first use two heuristics to extract from the dataset the questions that are related to security based on the tags of the posts. And then we use an advanced topic model, Latent Dirichlet Allocation (LDA) tuned using Genetic Algorithm (GA), to cluster different security-related questions based on their texts. After obtaining the different topics of security-related questions, we use their metadata to make various analyses. We summarize all the topics into five main categories, and investigate the popularity and difficulty of different topics as well. Based on the results of our study, we conclude several implications for researchers, educators and practitioners.},
journal = {Journal of Computer Science and Technology},
pages = {910--924},
volume = {31},
publisher = {Springer Science & Business Media},
number = {5},
year = {2016},
title = {What Security Questions Do Developers Ask? A Large-Scale Study of Stack Overflow Posts},
language = {eng},
address = {Beijing},
author = {Yang, Xin-Li and Lo, David and Xia, Xin and Wan, Zhi-Yuan and Sun, Jian-Ling},
keywords = {Computer Security ; Information Technology ; Analysis ; Studies ; Genetic Algorithms ; Software Engineering ; Security ; Stack Overflow ; Empirical Study ; Topic Model},
url = {http://search.proquest.com/docview/1816805183/},
}

@inproceedings{HillEmily2007Etnw,
series = {ASE '07},
abstract = {<p><p>Completing software maintenance and evolution tasks for todayâ s large, complex software systems can be difficult, often requiring considerable time to understand the system well enough to make correct changes. Despite evidence that successful programmers use program structure <i>as well as</i> identifier names to explore software, most existing program exploration techniques use either structural <i>or</i> lexical identifier information. By using only one type of information, automated tools ignore valuable clues about a developer's intentions - clues critical to the human program comprehension process. In this paper, we present and evaluate a technique that exploits <i>both</i> program structure and lexical information to help programmers more effectively explore programs. Our approach uses structural information to focus automated program exploration and lexical information to prune irrelevant structure edges from consideration. For the important program exploration step of expanding from a seed, our experimental results demonstrate that an integrated lexical-and structural-based approach is significantly more effective than a state-of-the-art structural program exploration technique</p></p>},
pages = {14--23},
publisher = {ACM},
booktitle = {Proceedings of the twenty-second IEEE/ACM international conference on automated software engineering},
isbn = {9781595938824},
year = {2007},
title = {Exploring the neighborhood with dora to expedite software maintenance},
language = {eng},
author = {Hill, Emily and Pollock, Lori and Vijay-Shanker, K},
keywords = {Natural Language Program Analysis ; Program Exploration ; Software Maintenance ; Software Tools ; Computer Science},
}

@article{HouraliMaryam2012UANN,
issn = {1877-7058},
abstract = {Ontology is one of the fundamental cornerstones of the semantic Web. The pervasive use of ontologies in information sharing and knowledge management calls for efficient and effective approaches to ontology development. Ontology learning, which seeks to discover ontological knowledge from various forms of data automatically or semiautomatically, can overcome the bottleneck of ontology acquisition in ontology development.. In this article a novel automated method for ontology learning is proposed. First, domain-related documents were collected. Secondly, the C-value method was implemented for extracting meaningful terms from documents. Then, an ART neural network was used to cluster documents, and terms’ weight was calculated by TF–IDF method in order to find candidate keyword for each cluster. Next, the Bayesian network and lexico-syntactic patterns were applied to construct the initial ontology. Finally, the proposed ontology was evaluated by expert's views and using the ontology for query expansion purpose. The primary results show that the proposed ontology learning method has higher precision than similar studies.},
journal = {Procedia Engineering},
pages = {3914--3923},
volume = {29},
publisher = {Elsevier Ltd},
year = {2012},
title = {Using ART2 Neural Network and Bayesian Network for Automating the Ontology Constructing Process},
language = {eng},
author = {Hourali, Maryam and Montazer, Gholam Ali},
keywords = {Ontology ; Art Neural Network ; Term Frequency–Inverse Document Frequency (Tf-Idf) ; C-Value Method ; Bayesian Network ; Lexico-Syntactic Patterns},
}

@misc{GrantScott2012Utmt,
abstract = {Latent topic models are statistical structures in which a "latent topic" describes some relationship between parts of the data. Co-maintenance is defined as an observable property of software systems under source control in which source code fragments are modified together in some time frame. When topic models are applied to software systems, latent topics emerge from code fragments. However, it is not yet known what these latent topics mean. In this research, we analyse software maintenance history, and show that latent topics often correspond to code fragments that are maintained together. Moreover, we show that latent topic models can identify such co-maintenance relationships even with no supervision. We can use this correlation both to categorize and understand maintenance history, and to predict future co-maintenance in practice. The relationship between co-maintenance and topics is directly analysed within changelists, with respect to both local pairwise code fragment similarity and global system-wide fragment similarity. This analysis is used to evaluate topic models used with a domain-specific programming language for web service similarity detection, and to estimate appropriate topic counts for modelling source code.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780499281555},
year = {2012},
title = {Using topic models to support software maintenance},
language = {eng},
author = {Grant, Scott},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Latent Topic Models ; Software Engineering},
url = {http://search.proquest.com/docview/1512650444/},
}

@article{GaoKe2011Mcad,
issn = {0020-7160},
abstract = {ASIFT: A new framework for fully affine invariant image comparisonDistinctive image features from scale invariant keypoints Invariant features extraction is important for object detection. Affine-SIFT (ASIFT) [J.M. Morel and G. Yu, ASIFT: A new framework for fully affine invariant image comparison , SIAM J. Imaging Sci. 2(2) (2009)] has been proved to be fully affine-invariant. However, the high cost of memory and query time hampers its application in large-scale object detection tasks. In this paper, we present a novel algorithm for mining concise and distinctive invariant features called affine-stable characteristics (ASC). Two new notions, global stability and local stability, are introduced to calculate the robustness of each feature from two mutually complementary aspects. Furthermore, to make these stable characteristics more distinctive, spatial information taken from several representative scales is encoded in a concise method. Experiments show that the robustness of our ASC is comparable with ASIFT, while the cost of memory can be reduced significantly to only 5%. Moreover, compared with the traditional SIFT method [D. Lowe, Distinctive image features from scale invariant keypoints , Int. J. Comput. Vis. 60(2) (2004), pp. 91–110], the accuracy of object detection can be improved 38.6% by our ASC using similar amount of features.},
journal = {International Journal of Computer Mathematics},
pages = {3953--3962},
volume = {88},
publisher = {Taylor & Francis},
number = {18},
year = {2011},
title = {Mining concise and distinctive affine-stable features for object detection in large corpus},
language = {eng},
author = {Gao, Ke and Zhang, Yongdong and Zhang, Wei and Lin, Shouxun},
keywords = {Object Detection ; Feature Extraction ; Affine-Stable Features ; Data Mining ; Spatial Information ; 62h35 ; 68t10},
}

@inproceedings{MartieLee2015Saei,
series = {MSR '15},
abstract = {<p><p>To date, most dedicated code search engines use ranking algorithms that focus only on the relevancy between the query and the results. In practice, this means that a developer may receive search results that are all drawn from the same project, all implement the same algorithm using the same external library, or all exhibit the same complexity or size, among other possibilities that are less than ideal. In this paper, we propose that code search engines should also locate both diverse and concise (brief but complete) sets of code results. We present four novel algorithms that use relevance, diversity, and conciseness in ranking code search results. To evaluate these algorithms and the value of diversity and conciseness in code search, twenty-one professional programmers were asked to compare pairs of top ten results produced by competing algorithms. We found that two of our new algorithms produce top ten results that are strongly preferred by the programmers.</p></p>},
pages = {76--87},
publisher = {IEEE Press},
booktitle = {Proceedings of the 12th Working Conference on mining software repositories},
isbn = {9780769555942},
year = {2015},
title = {Sameness: an experiment in code search},
language = {eng},
author = {Martie, Lee and van Der Hoek, André},
keywords = {Code ; Concise ; Diversity ; Results ; Sameness ; Search ; Similarity ; Top Ten},
}

@inproceedings{DurãoFrederico2008Aasl,
series = {SAC '08},
abstract = {<p><p>A fundamental principle for reusing software assets is providing means to access them. Information retrieval mechanisms assisted by semantic initiatives, play a very important role in finding relevant reusable assets. In this context, this paper presents a semantic search tool in order to improve the precision of search returns. Furthermore, the requirements, the decomposition of architectural module and aspects of implementation are presented.</p></p>},
pages = {1151--1157},
publisher = {ACM},
booktitle = {Proceedings of the 2008 ACM symposium on applied computing},
isbn = {9781595937537},
year = {2008},
title = {Applying a semantic layer in a source code search tool},
language = {eng},
author = {Durão, Frederico and Vanderlei, Taciana and Almeida, Eduardo and de L. Meira, Silvio},
keywords = {Automatic Classification ; Information Retrieval ; Ontology ; Search ; Semantic ; Source Code ; Computer Science},
}

@inproceedings{XuBowen2016Dcrq,
series = {MSR '16},
abstract = {<p><p>In software development process, developers often seek solutions to the technical problems they encounter by searching relevant questions on Q&A sites. When developers fail to find solutions on Q&A sites in their native language (e.g., Chinese), they could translate their query and search on the Q&A sites in another language (e.g., English). However, developers who are non-native English speakers often are not comfortable to ask or search questions in English, as they do not know the proper translation of the Chinese technical words into the English technical words. Furthermore, the process of manually formulating cross-language queries and determining the weight of query words is a tedious and time-consuming process.</p> <p>For the purpose of helping Chinese developers take advantage of the rich knowledge base of the English version of Stack Overflow and simplify the retrieval process, we propose an automated cross-language relevant question retrieval <i>(CLRQR)</i> system to retrieve relevant English questions on Stack Overflow for a given Chinese question. Our <i>CLRQR</i> system first extracts essential information (both Chinese and English) from the title and description of the input Chinese question, then performs domain-specific translation of the essential Chinese information into English, and formulates a query with highest-scored English words for retrieving relevant questions in a repository of 684,599 Java questions in English from Stack Overflow. To evaluate the performance of our proposed approach, we also propose four online retrieval approaches as baselines. We randomly select 80 Java questions in SegmentFault and V2EX (two Chinese Q&A websites for computer programming) as the query Chinese questions. Each approach returns top-10 most relevant questions for a given Chinese question. We invite 5 users to evaluate the relevance of the retrieved English questions. The experiment results show that <i>CLRQR</i> system outperforms the four baseline approaches, and the statistical tests show the improvements are significant.</p></p>},
pages = {413--424},
publisher = {ACM},
booktitle = {Proceedings of the 13th International Conference on mining software repositories},
isbn = {9781450341868},
year = {2016},
title = {Domain-specific cross-language relevant question retrieval},
language = {eng},
author = {Xu, Bowen and Xing, Zhenchang and Xia, Xin and Lo, David and Wang, Qingye and Li, Shanping},
keywords = {Cross-Language Question Retrieval ; Domain-Specific Translation},
}

@article{YangJinqiu2014SIsr,
issn = {1382-3256},
abstract = {Code search is an integral part of software development and program comprehension. The difficulty of code search lies in the inability to guess the exact words used in the code. Therefore, it is crucial for keyword-based code search to expand queries with semantically related words, e.g., synonyms and abbreviations, to increase the search effectiveness. However, it is limited to rely on resources such as English dictionaries and WordNet to obtain semantically related words in software because many words that are semantically related in software are not semantically related in English. On the other hand, many words that are semantically related in English are not semantically related in software. This paper proposes a simple and general technique to automatically infer semantically related words (referred to as rPairs) in software by leveraging the context of words in comments and code. In addition, we propose a ranking algorithm on the rPair results and study cross-project rPairs on two sets of software with similar functionality, i.e., media browsers and operating systems. We achieve a reasonable accuracy in nine large and popular code bases written in C and Java. Our further evaluation against the state of art shows that our technique can achieve a higher precision and recall. In addition, the proposed ranking algorithm improves the rPair extraction accuracy by bringing correct rPairs to the top of the list. Our cross-project study successfully discovers overlapping rPairs among projects of similar functionality and finds that cross-project rPairs are more likely to be correct than project-specific rPairs. Since the cross-project rPairs are highly likely to be general for software of the same type, the discovered overlapping rPairs can benefit other projects of the same type that have not been analyzed.},
journal = {Empirical Software Engineering},
pages = {1856--1886},
volume = {19},
publisher = {Springer US},
number = {6},
year = {2014},
title = {SWordNet: Inferring semantically related words from software context},
language = {eng},
address = {Boston},
author = {Yang, Jinqiu and Tan, Lin},
keywords = {Semantically related words ; Code search ; Program comprehension},
}

@article{LinWei-Chao2016Sasa,
issn = {0264-0473},
abstract = {<p> Purpose This paper aims to introduce a prototype system called SAFQuery (Simple And Flexible Query interface). In many existing Web search interfaces, simple and advanced query processes are treated separately that cannot be issued interchangeably. In addition, after several rounds of queries for specific information need(s), it is possible that users might wish to re-examine the retrieval results corresponding to some previous queries or to slightly modify some of the specific queries issued before. However, it is often hard to remember what queries have been issued. These factors make the current Web search process not very simple or flexible. Design/methodology/approach In SAFQuery, the simple and advanced query strategies are integrated into a single interface, which can easily formulate query specifications when needed in the same interface. Moreover, query history information is provided that displays the past query specifications, which can help with the memory load. Findings The authors' experiments by user evaluation show that most users had a positive experience when using SAFQuery. Specifically, it is easy to use and can simplify the Web search task. Originality/value The proposed prototype system provides simple and flexible Web search strategies. Particularly, it allows users to easily issue simple and advanced queries based on one single query interface, interchangeably. In addition, users can easily input previously issued queries without spending time to recall what the queries are and/or to re-type previous queries. </p>},
journal = {The Electronic Library},
pages = {155--168},
volume = {34},
publisher = {Emerald Group Publishing Limited},
number = {1},
year = {2016},
title = {SAFQuery: a simple and flexible advanced Web search interface},
language = {eng},
author = {Lin, Wei-Chao and Ke, Shih-Wen and Tsai, Chih-Fong},
keywords = {Information Retrieval ; Interfaces ; Computer Science ; Library & Information Science},
}

@inproceedings{SatterAbdus2017ASMR,
series = {Programming '17},
abstract = {<p><p>The effectiveness of a code search engine is reduced if semantically similar code fragments are not indexed under common and proper terms. In this paper, a technique named Feature-Wise Similar Method Finder (FWSMF) is proposed which checks functional similarity among codes by executing and matching outputs against the same set of inputs. It then determines appropriate index terms by finding keywords that are found in most of the code snippets. As a result, code fragments that contain different keywords but implement the same feature, can be retrieved all together. Experimental analysis shows that on an average, FWSMF produces 61% and 29% more precision than two existing techniques named Keyword Based Code Search (KBCS) and Interface Driven Code Search (IDCS) respectively. It also shows 34% and 55% more recall than KBCS and IDCS correspondingly. It retrieves self executable code snippets which can be easily pluggable in the intended development context and thus reduces time and effort while reusing code.</p></p>},
pages = {1--3},
volume = {129681},
publisher = {ACM},
booktitle = {Companion to the first International Conference on the Art, Science and Engineering of Programming},
isbn = {9781450348362},
year = {2017},
title = {A Similarity-Based Method Retrieval Technique to Improve Effectiveness in Code Search},
language = {eng},
author = {Satter, Abdus and Sakib, Kazi},
keywords = {Code Search ; Self-Executable Method ; Software Reuse},
}

@article{WuDan2017Iodo,
issn = {0264-0473},
abstract = {<p> Purpose This paper discusses the differences in search pattern transitions for mobile phone, tablet and desktop devices by mining the transaction log data of a library online public access catalogue (OPAC). We aimed to analyze the impacts of different devices on user search behavior and provide constructive suggestions for the development of library OPACs on different devices. Design/methodology/approach Based on transaction logs which are 9 GB in size and contain 16,140,509 records of a university library OPAC, statistics and clustering were used to analyze the differences in search pattern transitions on different devices in terms of two aspects: search field transition patterns and query reformulation patterns. Findings Search field transition patterns are influenced by the input function and user interfaces of different devices. As reformulation times increase, the differences between query reformulation patterns among different devices decrease. Practical implications Mobile-side libraries need to optimize user interfaces, for example by setting web page labels and improving input capabilities. Desk-side libraries can add more suggestive content on the interface. Originality/value Unlike previous studies, which have analyzed web search, this paper focuses on library OPAC search. The search function of mobile phones, tablets and desktops were found to be asymptotic, which was a good illustration of how devices have a large impact on user search behavior. </p>},
journal = {The Electronic Library},
pages = {650--666},
volume = {35},
publisher = {Emerald Publishing Limited},
number = {4},
year = {2017},
title = {Impact of device on search pattern transitions},
language = {eng},
author = {Wu, Dan and Bi, Renmin},
keywords = {User Interfaces ; Information Seeking Behaviors ; Transaction Logs ; Library Opac Searches ; Query Reformulation ; Search Fields ; Search Patterns ; Computer Science ; Library & Information Science},
}

@misc{HillEmily2010Inla,
abstract = {Today's software is large and complex, with systems consisting of millions of lines of code. New developers to a software project face significant challenges in locating code related to their maintenance tasks of fixing bugs or adding new features. Developers can simply be assigned a bug and told to fix it—even when they have no idea where to begin. In fact, research has shown that a developer typically spends more time locating and understanding code during maintenance than modifying it. We can significantly reduce the cost of software maintenance by reducing the time and effort to find and understand the code relevant to a software maintenance task. In this dissertation, we demonstrate how textual and structural information in source code can be used to improve software search and exploration tools. To facilitate integration of this information into additional software tools, we present a novel model of word usage in software. This model provides software engineering tool designers access to both structural and linguistic information about the source code, where previously only structural information was available. We utilize textual and structural information to improve software search and program exploration tools, and evaluate against competing state of the art approaches. Our evaluations show that combining textual and structural information can outperform competing state of the art techniques. Finally, we outline uses of the model to improve software engineering tools beyond program search and exploration.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781124241951},
year = {2010},
title = {Integrating natural language and program structure information to improve software search and exploration},
language = {eng},
author = {Hill, Emily},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Information ; Natural Language ; Program Exploration ; Program Structure ; Software Maintenance ; Software Search},
url = {http://search.proquest.com/docview/759929332/},
}

@inproceedings{KramárTomaš2012Dsaa,
series = {RecSys '12},
abstract = {<p><p>Narrowing down the context in the ranking phase of information retrieval has been shown to produce results that are more relevant to searcher's need. We have identified two types of contexts that could be used in the process of personalisation. We research these contexts in the domain of personalised search, but show that our approach can be used for any kind of personalisation or recommendation. We focus on two aspects of the context: temporal context and activity-based context and describe a more general personalisation framework based on lightweight semantics, that can leverage any type of context.</p></p>},
pages = {321--324},
publisher = {ACM},
booktitle = {Proceedings of the sixth ACM conference on recommender systems},
isbn = {9781450312707},
year = {2012},
title = {Dynamically selecting an appropriate context type for personalisation},
language = {eng},
author = {Kramár, Tomaš and Bieliková, Mária},
keywords = {Search Context ; Engineering},
}

@article{PooDannyC.C.2000Daio,
issn = {0169-023X},
abstract = {An expert system Web interface to online catalogs called E-Referencer is being developed. An initial prototype has been implemented. The interface has a repertoire of initial search strategies and reformulation strategies that it selects and implements to help users retrieve relevant records. It uses the Z39.50 protocol to access library systems on the Internet. This paper describes the design and implementation of the E-Referencer. A preliminary evaluation of the strategies is also presented.},
journal = {Data & Knowledge Engineering},
pages = {199--218},
volume = {32},
publisher = {Elsevier B.V.},
number = {2},
year = {2000},
title = {Design and implementation of the E-referencer},
language = {eng},
author = {Poo, Danny C.C. and Toh, Teck-Kang and Khoo, Christopher S.G.},
keywords = {Online Cataloge Searches ; Information Rerieval ; Knowledge-Based Approach ; Expert System ; Z39.50 Standard},
}

@misc{EddyBrian2015Utsl,
abstract = {Software maintenance and evolution make up a considerable portion of the time and effort spent during the life cycle of a software system. During the maintenance and evolution phase, the majority of a developer’s time is spent on program comprehension tasks. Feature location (i.e., identifying a starting point for a change), impact analysis (i.e., identifying all source elements involved in a change), and software summarization (i.e., automatically summarizing the responsibilities of a source element) are examples of such tasks. Recent research in these areas has focused on improving each process to ease the burden on developers and decrease the time spent in each task through the use of textual information, dependency graphs, and execution traces. Furthermore, the success of text retrieval in other areas (e.g., traceability) has initiated new studies in automating feature location by the use of text retrieval techniques, such as the vector space model (VSM), latent semantic indexing (LSI), and latent Dirichlet allocation (LDA). Some research has been done to improve LSI and VSM models by combining structural information (i.e., information regarding the creation and use of objects and methods within the code) with the corpus obtained from extracting text from source code. However, little research has focused on improving LDA and more sophisticated topic models (i.e., a statistical model of the abstract topics that occur in a corpus) with structural information. Furthermore, no study has looked at how a developer’s knowledge of a software system’s structure may be incorporated into text retrieval based feature location for software maintenance tasks. The research presented in this dissertation makes two main contributions. First, it evaluates a methodology for incorporating structural information into the corpus obtained in the text extraction phase by modifying the weights of terms based on their importance to the individual source elements. Furthermore, this dissertation introduces a novel technique for performing structured text retrieval that allows developers to use their existing knowledge about the structure of a software system. This dissertation is organized into the following parts: a demonstration of the effects of structural weighting schemes on the effectiveness of topic-modeling in feature location, the introduction of a new structured source code retrieval model, a demonstration of the effects of structured queries on the effectiveness of structured source code retrieval for feature location, and additional insights into how and when these approaches should be incorporated.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781339106144},
year = {2015},
title = {Using the structural location of terms to improve the results of text retrieval based approaches to feature location},
language = {eng},
author = {Eddy, Brian},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Latent Dirichlet Allocation ; Latent Semantic Indexing ; Software Maintenance ; Vector Space Model},
url = {http://search.proquest.com/docview/1733230966/},
}

@article{ZhangTao2016ALRo,
issn = {0010-4620},
abstract = {Due to the increasing scale and complexity of software products, software maintenance especially on bug resolution has become a challenging task. Generally in large-scale software programs, developers depend on software artifacts (e.g., bug report, source code and change history) in bug repositories to complete the bug resolution task. However, a mountain of submitted bug reports every day increase the developers' workload. Therefore, ‘How to effectively resolve software defects by utilizing software artifacts?’ becomes a research hotspot in software maintenance. Considerable studies have been done on bug resolution by using multi-techniques, which cover data mining, machine learning and natural language processing. In this paper, we present a literature survey on tasks, challenges and future directions of bug resolution in software maintenance process. Our investigation concerns the most important phases in bug resolution, including bug understanding, bug triage and bug fixing. Moreover, we present the advantages and disadvantages of each study. Finally, based on the investigation and comparison results, we propose the future research directions of bug resolution.},
journal = {The Computer Journal},
pages = {741--773},
volume = {59},
publisher = {Oxford University Press},
number = {5},
year = {2016},
title = {A Literature Review of Research in Bug Resolution: Tasks, Challenges and Future Directions},
author = {Zhang, Tao and Jiang, He and Luo, Xiapu and Chan, Alvin T.S.},
keywords = {Bug Resolution ; Bug Report ; Software Maintenance ; Bug Understanding ; Bug Triage ; Bug Fixing},
}

@inproceedings{LiuDapeng2007Flvi,
series = {ASE '07},
abstract = {<p><p>The paper presents a semi-automated technique for feature location in source code. The technique is based on combining information from two different sources: an execution trace, on one hand and the comments and identifiers from the source code, on the other hand. </p> <p>Users execute a single partial scenario, which exercises the desired feature and all executed methods are identified based on the collected trace. The source code is indexed using Latent Semantic Indexing, an Information Retrieval method, which allows users to write queries relevant to the desired feature and rank all the executed methods based on their textual similarity to the query. </p> <p>Two case studies on open source software (JEdit and Eclipse) indicate that the new technique has high accuracy, comparable with previously published approaches and it is easy to use as it considerably simplifies the dynamic analysis.</p></p>},
pages = {234--243},
publisher = {ACM},
booktitle = {Proceedings of the twenty-second IEEE/ACM international conference on automated software engineering},
isbn = {9781595938824},
year = {2007},
title = {Feature location via information retrieval based filtering of a single scenario execution trace},
language = {eng},
author = {Liu, Dapeng and Marcus, Andrian and Poshyvanyk, Denys and Rajlich, Vaclav},
keywords = {Concept Location ; Dynamic and Static Analyses ; Feature Identification ; Information Retrieval ; Program Understanding ; Computer Science},
}

@article{RomeroM.2013AcoF,
issn = {0950-7051},
abstract = {•Efficient and automatic manage of large collection of documents.•No need of handcrafted knowledge modeling.•Highly-precise answers.•Improved language coverage of the possible wordings users could employ when querying the system.•Tag clouds as an interpretable representation mechanism and as a way to improve navigation and learning through the domain of knowledge. FAQ (Frequency Asked Questions) lists have attracted increasing attention for companies and organizations. There is thus a need for high-precision and fast methods able to manage large FAQ collections. In this context, we present a FAQ retrieval system as part of a FAQ exploiting project. Following the growing trend towards Web 2.0, we aim to provide users with mechanisms to navigate through the domain of knowledge and to facilitate both learning and searching, beyond classic FAQ retrieval algorithms. To this purpose, our system involves two different modules: an efficient and precise FAQ retrieval module and, a tag cloud generation module designed to help users to complete the comprehension of the retrieved information. Empirical results evidence the validity of our approach with respect to a number of state-of-the-art algorithms in terms of the most popular metrics in the field.},
journal = {Knowledge-Based Systems},
pages = {81--96},
volume = {49},
publisher = {Elsevier B.V.},
year = {2013},
title = {A cloud of FAQ: A highly-precise FAQ retrieval system for the Web 2.0},
language = {eng},
author = {Romero, M. and Moreo, A. and Castro, J.L.},
keywords = {FAQ Retrieval ; Wordnet ; Wikipedia Concepts ; Natural Language ; Tag Cloud},
}

@inproceedings{GrechanikMark2010Asef,
series = {ICSE '10},
abstract = {<p><p>A fundamental problem of finding applications that are highly relevant to development tasks is the mismatch between the high-level intent reflected in the descriptions of these tasks and low-level implementation details of applications. To reduce this mismatch we created an approach called <i>Exemplar (EXEcutable exaMPLes ARchive)</i> for finding highly relevant software projects from large archives of applications. After a programmer enters a natural-language query that contains high-level concepts (e.g., MIME, data sets), Exemplar uses information retrieval and program analysis techniques to retrieve applications that implement these concepts. Our case study with 39 professional Java programmers shows that Exemplar is more effective than Sourceforge in helping programmers to quickly find highly relevant applications.</p></p>},
pages = {475--484},
volume = {1},
publisher = {ACM},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on software engineering},
isbn = {9781605587196},
year = {2010},
title = {A search engine for finding highly relevant applications},
language = {eng},
author = {Grechanik, Mark and Fu, Chen and Xie, Qing and Mcmillan, Collin and Poshyvanyk, Denys and Cumby, Chad},
}

@misc{RevelleMeghan2010Sfsm,
abstract = {Software maintenance is the process of modifying a software system to fix defects, improve performance, add new functionality, or adapt the system to a new environment. A maintenance task is often initiated by a bug report or a request for new functionality. Bug reports typically describe problems with incorrect behaviors or functionalities. These behaviors or functionalities are known as features. Even in very well-designed systems, the source code that implements features is often not completely modularized. The delocalized nature of features makes maintaining them challenging. Since maintenance tasks are expressed in terms of features, the goal of this dissertation is to support software maintenance at the feature-level. We focus on two tasks in particular: feature location and impact analysis via feature coupling. Feature location is the process of identifying the source code that implements a feature, and it is an essential first step to any maintenance task. There are many existing techniques for feature location that incorporate various types of analyses such as static, dynamic, and textual. In this dissertation, we recognize the advantages of leveraging several types of analyses and introduce a new approach to feature location based on combining dynamic analysis, textual analysis, and web mining algorithms applied to software. The use of web mining for feature location is a novel contribution, and we show that our new techniques based on web mining are significantly more effective than the current state of the art. After using feature location to identify a feature's source code, maintenance can be completed on that feature. Impact analysis should then be performed to revalidate the system and determine which other features may have been affected by the modifications. We define three feature coupling metrics that capture the relationship between features based on structural information, textual information, and their combination. Our novel feature coupling metrics can be used for impact analysis to quantify the strength of coupling between pairs of features. We performed three empirical studies on open-source software systems to assess the feature coupling metrics and established three major results. First, there is a moderate to strong statistically significant correlation between feature coupling and faults. Second, feature coupling can be used to correctly determine about half of the other features that would be affected by a change to a given feature. Finally, we found that the metrics align with developers' opinions about pairs of features that are actually coupled.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781124182568},
year = {2010},
title = {Supporting feature-level software maintenance},
language = {eng},
author = {Revelle, Meghan},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Data Fusion ; Feature Coupling ; Feature Location ; Information Retrieval ; Software Maintenance ; Source Code ; Web Mining},
url = {http://search.proquest.com/docview/750149631/},
}

@inproceedings{LiZhixing2016Qrbl,
series = {Internetware '16},
abstract = {<p><p>The Internet-scale open source software (OSS) production in various communities are generating abundant reusable resources for software developers. However, how to retrieve and reuse the desired and mature software from huge amounts of candidates is a great challenge: there are usually big gaps between the user application contexts (that often used as queries) and the OSS key words (that often used to match the queries). In this paper, we define the scenario-based query problem for OSS retrieval, and then we propose a novel approach to reformulate the raw query by leveraging the crowd wisdom from millions of developers to improve the retrieval results. We build a software-specific domain lexical database based on the knowledge in open source communities, by which we can expand and optimize the input queries. The experiment results show that, our approach can reformulate the initial query effectively and outperforms other existing search engines significantly at finding mature software.</p></p>},
pages = {36--44},
volume = {18-},
publisher = {ACM},
booktitle = {Proceedings of the 8th Asia-Pacific Symposium on internetware},
isbn = {9781450348294},
year = {2016},
title = {Query reformulation by leveraging crowd wisdom for scenario-based software search},
language = {eng},
author = {Li, Zhixing and Wang, Tao and Zhang, Yang and Zhan, Yun and Yin, Gang},
keywords = {Crowd Wisdom ; Query Reformulation ; Software Retrieval},
}

@article{SimkoJakub2011Sdvh,
issn = {1552-6283},
abstract = {The effective acquisition of (semantic) metadata is crucial for many present day applications. Games with a purpose address this issue by transforming computational problems into computer games. The authors present a novel approach to metadata acquisition via Little Search Game (LSG)--a competitive web search game, whose purpose is the creation of a term relationship network. From a player perspective, the goal is to reduce the number of search results returned for a given search term by adding negative search terms to a query. The authors describe specific aspects of the game's design, including player motivation and anti-cheating issues. The authors have performed a series of experiments with Little Search Game, acquired real-world player input, gathered qualitative feedback from the players, constructed and evaluated term relationship network from the game logs and examined the types of created relationships. Keywords: Crowdsourcing, Games with a Purpose, Human Computation, Metadata, Semantics, Search Query, Term Network, Web Search},
journal = {International Journal on Semantic Web and Information Systems},
volume = {7},
publisher = {IGI Global},
number = {3},
year = {2011},
title = {Semantics discovery via human computation games.(Report)},
language = {English},
author = {Simko, Jakub and Tvarozek, Michal and Bielikova, Maria},
keywords = {Metadata -- Research ; Semantics -- Research ; Data Collection -- Methods ; Game Theory -- Research},
}

@article{XieIris2010TftF,
issn = {1999-5903},
abstract = {In their web search processes users apply multiple types of search strategies, which consist of different search tactics. This paper identifies eight types of information search strategies with associated cases based on sequences of search tactics during the information search process. Thirty-one participants representing the general public were recruited for this study. Search logs and verbal protocols offered rich data for the identification of different types of search strategies. Based on the findings, the authors further discuss how to enhance web-based information retrieval (IR) systems to support each type of search strategy.},
journal = {Future Internet},
pages = {259--281},
volume = {2},
publisher = {MDPI AG},
number = {3},
year = {2010},
title = {Tales from the Field: Search Strategies Applied in Web Searching},
language = {eng},
address = {Basel},
author = {Xie, Iris and Joo, Soohyung},
keywords = {United States–Us ; Studies ; Internet ; Web Browsers ; End Users ; Search Engines ; Psychological Aspects ; United States ; Experimental/Theoretical ; Telecommunications Systems & Internet Communications ; Software & Systems ; Social Trends & Culture ; Search Strategies ; Search Tactics ; Web Environment ; IR System Design},
url = {http://search.proquest.com/docview/1524878273/},
}

@misc{ThomasStephen2013Musr,
abstract = {Mining Software Repositories, which is the process of analyzing the data related to software development practices, is an emerging field which aims to aid development teams in their day to day tasks. However, data in many software repositories is currently unused because the data is unstructured, and therefore difficult to mine and analyze. Information Retrieval (IR) techniques, which were developed specifically to handle unstructured data, have recently been used by researchers to mine and analyze the unstructured data in software repositories, with some success. The main contribution of this thesis is the idea that the research and practice of using IR models to mine unstructured software repositories can be improved by going beyond the current state of affairs. First, we propose new applications of IR models to existing software engineering tasks. Specifically, we present a technique to prioritize test cases based on their IR similarity, giving highest priority to those test cases that are most dissimilar. In another new application of IR models, we empirically recover how developers use their mailing list while developing software. Next, we show how the use of advanced IR techniques can improve results. Using a framework for combining disparate IR models, we find that bug localization performance can be improved by 14–56% on average, compared to the best individual IR model. In addition, by using topic evolution models on the history of source code, we can uncover the evolution of source code concepts with an accuracy of 87–89%. Finally, we show the risks of current research, which uses IR models as black boxes without fully understanding their assumptions and parameters. We show that data duplication in source code has undesirable effects for IR models, and that by eliminating the duplication, the accuracy of IR models improves. Additionally, we find that in the bug localization task, an unwise choice of parameter values results in an accuracy of only 1%, where optimal parameters can achieve an accuracy of 55%. Through empirical case studies on real-world systems, we show that all of our proposed techniques and methodologies significantly improve the state-of-the-art.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780499280008},
year = {2013},
title = {Mining unstructured software repositories using IR models},
language = {eng},
author = {Thomas, Stephen},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Data Mining ; Empirical Studies ; Information Retrieval ; Machine Learning ; Mining Software Repositories ; Software Engineering},
url = {http://search.proquest.com/docview/1511978751/},
}

@misc{Al-ShannaqMoyAwiah2015Afei,
abstract = {The research explores the use of advanced Semantic Web tools to problems in Information Retrieval (IR). Specifically, the IR methods that are used to retrieve information from the documents depending on traditional keyword based suffer from some drawbacks. Among these drawbacks are word sense ambiguity, the query intent ambiguity, and the ability to exploit semantic knowledge within documents. These drawbacks can negatively affect the precision. The work advances the field by investigating approaches to discover the abstract "topics" that occur in a collection of documents based on topic modeling approach, then expand the query to explore and discover hidden relations between the documents. The work will be applied to Arabic and English documents. The hypothesis is that classifying the corpus with meaningful descriptive information, and then expanding the query by using Automatic Query Expansion, will improve the results of the IR methods for indexing and querying information. In particular, the work uses topic model techniques which are considered as advanced Information Retrieval methods that have been widely used for indexing and analyzing the corpus and then apply the advanced Semantic Web techniques in order to increase the accuracy. These techniques will be tested on both English and Arabic documents. Recent topic modeling techniques with query expansion are discussed and compared for problems such as efficiency and scaling.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781369241594},
year = {2015},
title = {Algorithms for enhancing information retrieval using Semantic Web},
language = {eng},
author = {Al-Shannaq, Moy'Awiah},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Algorithms ; Information ; Retrieval ; Semantic Web},
url = {http://search.proquest.com/docview/1837086763/},
}

@inproceedings{WangShaowei2014Acsi,
series = {ASE '14},
abstract = {<p><p>Code search techniques return relevant code fragments given a user query. They typically work in a passive mode: given a user query, a <i>static</i> list of code fragments sorted by the relevance scores decided by a code search technique is returned to the user. A user will go through the sorted list of returned code fragments from top to bottom. As the user checks each code fragment one by one, he or she will naturally form an opinion about the true relevance of the code fragment. In an active model, those opinions will be taken as feedbacks to the search engine for refining result lists.</p> <p>In this work, we incorporate users' opinion on the results from a code search engine to refine result lists: as a user forms an opinion about one result, our technique takes this opinion as feedback and leverages it to re-order the results to make truly relevant results appear earlier in the list. The refinement results can also be cached to potentially improve future code search tasks. We have built our active refinement technique on top of a state-of-the-art code search engine---Portfolio. Our technique improves Portfolio in terms of Normalized Discounted Cumulative Gain (NDCG) by more than 11.3%, from 0.738 to 0.821.</p></p>},
pages = {677--682},
publisher = {ACM},
booktitle = {Proceedings of the 29th ACM/IEEE international conference on automated software engineering},
isbn = {9781450330138},
year = {2014},
title = {Active code search: incorporating user feedback to improve code search relevance},
language = {eng},
author = {Wang, Shaowei and Lo, David and Jiang, Lingxiao},
keywords = {Active Learning ; Code Search ; User Feedback ; Computer Science},
}

@misc{KwakThomas2017IMPf,
abstract = {To aid programmers in searching for code on the Internet, researchers and developers have created code search engines. These search engines use ranking algorithms to sort their results based on properties of the code. However, obtaining the properties to use a ranking algorithm requires a resource-intensive process, including downloading, parsing, analyzing, and indexing large amounts of code. Additionally, few guidelines exist for reducing the resources needed for this process without affecting the performance of the ranking algorithm. We explore two techniques for improving the process of mining code from the Internet for code search engines. First, we introduce What-If Ranking Analysis, a novel technique that attempts to find cheaper versions of a ranking algorithm by reducing the number of properties required when ranking. Second, we modify the original Beowulf cluster to optimize on network throughput instead of CPU performance to examine whether the modified cluster can outperform a single high-performance server in mining and uploading code for use in a code search engine. Our findings show that more efficient ranking algorithms exist that perform as well as the original ranking algorithm while reducing the time spent mining by 44% and reducing the disk space used for storage by 41%. Further, we find that the modified Beowulf cluster mines and processes code three times faster than a high-performance server at approximately the same cost.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780355307528},
year = {2017},
title = {Improving Mining Performance for Internet Code Search Engines},
language = {eng},
author = {Kwak, Thomas},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Code ; Mining ; Ranking ; Search ; Source ; What-If},
url = {http://search.proquest.com/docview/1978496365/},
}

@inproceedings{ShepherdDavid2007Unlp,
series = {AOSD '07},
abstract = {<p>Most current software systems contain undocumented high-level ideas implemented across multiple files and modules. When developers perform program maintenance tasks, they often waste time and effort locating and understanding these scattered concerns. We have developed a semi-automated concern location and comprehension tool, Find-Concept, designed to reduce the time developers spend on maintenance tasks and to increase their confidence in the results of these tasks. Find-Concept is effective because it searches a unique natural language-based representation of source code, uses novel techniques to expand initial queries into more effective queries, and displays search results in an easy-to-comprehend format. We describe the Find-Concept tool, the underlying program analysis, and an experimental study comparing Find-Concept's search effectiveness with two state-of-the-art lexical and information retrieval-based search tools. Across nine action-oriented concern location tasks derived from open source bug reports, our Eclipse-based tool produced more effective queries more consistently than either competing search tool with similar user effort.</p>},
pages = {212--224},
volume = {208},
publisher = {ACM},
booktitle = {Proceedings of the 6th international conference on aspect-oriented software development},
isbn = {1595936157},
year = {2007},
title = {Using natural language program analysis to locate and understand action-oriented concerns},
language = {eng},
author = {Shepherd, David and Fry, Zachary and Hill, Emily and Pollock, Lori and Vijay-Shanker, K},
keywords = {Feature Location ; Program Analysis ; Remodularization ; Reverse Engineering ; Computer Science},
}

@misc{WangShaowei2015MCS,
abstract = {Today’s software is large and complex, consisting of millions of lines of code. New developers of a software project always face significant challenges in finding code related to their development or maintenance tasks (e.g., implementing features, fixing bugs and adding new features). In fact, research has shown that developers typically spend more time on locating and understanding code than modifying it. Thus, we can significantly reduce the cost of software development and maintenance by reducing the time to search and understand code relevant to a software development or maintenance task. In order to reduce the time of searching and understanding relevant code, many code search techniques are proposed. For different circumstances, the best form of inputs (i.e., queries) users can provide to search for a piece of code of interest may differ. During development, developers usually like to search a piece of code implementing certain functionality for reuse by expressing their queries in free-form texts (i.e., natural language). After deployment, users might report bugs to an issue tracking system. For these bug reports, developers would benefit from an automated tool that can identify buggy code from the descriptions of the symptoms of the bugs. During maintenance, developers may notice that some pieces of code with a particular structure are potentially buggy. A code search technique that allows users to specify the code structure using a query language may be the best choice. In another scenario, developers may have found some buggy code examples and they would like to locate other similar code snippets containing the same problem across the entire system. In this case, a code search technique that takes as input known buggy code examples is the best choice. During testing, suppose developers have execution traces of a suite of test cases, they might want to use these execution traces as input to search the buggy code. Developers may also like to provide feedback to the code search engine to improve results. From the above examples, we could see that there is a need for multimodal code search which allows users to express their needs in multiple input forms and processes different inputs with different strategies. This will make their search more convenient and effective. In this dissertation, we propose a multimodal code search engine, which employs novel techniques that allow developers to effectively find code elements of interest by processing developers’ inputs in various input forms including free-form texts, an SQL-like domain-specific language, code examples, execution traces, and user feedback. In the multimodal code search engine, we utilize program analysis, data mining, and machine learning techniques to improve the code search accuracy. Our evaluations show that our approaches improve over state-of-the-art approaches significantly.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781339188560},
year = {2015},
title = {Multimodal Code Search},
language = {eng},
author = {Wang, Shaowei},
keywords = {Computer Science ; Computer Science ; Applied Sciences},
url = {http://search.proquest.com/docview/1724726925/},
}

@misc{WangShaowei2015MCS,
abstract = {Today’s software is large and complex, consisting of millions of lines of code. New developers of a software project always face significant challenges in finding code related to their development or maintenance tasks (e.g., implementing features, fixing bugs and adding new features). In fact, research has shown that developers typically spend more time on locating and understanding code than modifying it. Thus, we can significantly reduce the cost of software development and maintenance by reducing the time to search and understand code relevant to a software development or maintenance task. In order to reduce the time of searching and understanding relevant code, many code search techniques are proposed. For different circumstances, the best form of inputs (i.e., queries) users can provide to search for a piece of code of interest may differ. During development, developers usually like to search a piece of code implementing certain functionality for reuse by expressing their queries in free-form texts (i.e., natural language). After deployment, users might report bugs to an issue tracking system. For these bug reports, developers would benefit from an automated tool that can identify buggy code from the descriptions of the symptoms of the bugs. During maintenance, developers may notice that some pieces of code with a particular structure are potentially buggy. A code search technique that allows users to specify the code structure using a query language may be the best choice. In another scenario, developers may have found some buggy code examples and they would like to locate other similar code snippets containing the same problem across the entire system. In this case, a code search technique that takes as input known buggy code examples is the best choice. During testing, suppose developers have execution traces of a suite of test cases, they might want to use these execution traces as input to search the buggy code. Developers may also like to provide feedback to the code search engine to improve results. From the above examples, we could see that there is a need for multimodal code search which allows users to express their needs in multiple input forms and processes different inputs with different strategies. This will make their search more convenient and effective. In this dissertation, we propose a multimodal code search engine, which employs novel techniques that allow developers to effectively find code elements of interest by processing developers’ inputs in various input forms including free-form texts, an SQL-like domain-specific language, code examples, execution traces, and user feedback. In the multimodal code search engine, we utilize program analysis, data mining, and machine learning techniques to improve the code search accuracy. Our evaluations show that our approaches improve over state-of-the-art approaches significantly.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781369239898},
year = {2015},
title = {Multimodal Code Search},
language = {eng},
author = {Wang, Shaowei},
keywords = {Information Science ; Information Science ; Communication and the Arts ; Inputs ; Lines of Code ; Software},
url = {http://search.proquest.com/docview/1837075995/},
}

@misc{WangXiaoran2017EAUG,
abstract = {Because resources for today's software are used primarily for maintenance and evolution, researchers are striving to make software engineers more efficient through automation. Programmers now use integrated development environments (IDEs), debuggers, and tools for code search, testing, and program understanding to reduce the tedious, error-prone tasks. A key component of these tools is analyzing source code and gathering information for software developers. Most analyses treat a method as a set of individual statements or a bag of words. Those analyses do not leverage information at levels of abstraction between the individual statement and the whole method. However, a method normally contains multiple high-level steps to achieve a certain function or execute an algorithm. The steps are expressed by a sequence of statements instead of a single statement. In this dissertation, I have explored the feasibility of automatically identifying these high level actions towards improving software maintenance tools and program understanding. Specifically, methods can often be viewed as a sequence of blocks that correspond to high level actions. We define an action unit as a code block that consists of a sequence of consecutive statements that logically implement a high level action. Rather than lower level actions represented by individual statements, action units represent a higher level action, for example, “initializing a collection”' or “setting up a GUI component”. Action units are intermediary steps of an algorithm or sub-actions of a bigger and more general action. In this dissertation, I (1) introduce the notion of action units and define the kinds of action units, (2) develop techniques to automatically identify actions for loop-based action units, (3) automatically generate natural language descriptions for object-related action units, and (4) automatically insert blank lines into methods based on action units to improve source code readability.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780355251975},
year = {2017},
title = {Exploring Action Unit Granularity of Source Code for Supporting Software Maintenance},
language = {eng},
author = {Wang, Xiaoran},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Code Readability ; Comment Generation ; Software Documentation ; Source Code Analysis},
url = {http://search.proquest.com/docview/1958957034/},
}

@misc{BajracharyaSushil2010Ficr,
abstract = {Internet-Scale code retrieval deals with the representation, storage, and access of relevant source code from a large amount of source code available on the Internet. Internet-Scale code retrieval systems support common emerging practices among software developers related to finding and reusing source code. In this dissertation we focus on some system and domain-specific challenges of Internet-Scale code retrieval. This dissertation starts with an in-depth study of how developers use Koders, a commercial code search engine. The results of this study highlight several problems that need to be tackled in a commercial code search engine. To build solutions for some of these problems we develop an infrastructure, Sourcerer, that includes models and tools for large-scale collection and analysis of open source code. The stored contents and set of programmable services in Sourcerer enable rapid development and evaluation of retrieval schemes and applications of code search. We demonstrate the feasibility of developing state-of-the-art Internet-Scale code retrieval techniques on top of Sourcerer by presenting the implementation and evaluation details of code-specific retrieval schemes and code search tools. The central premise of this dissertation is that source code retrieval techniques that incorporate structural information extracted from source code can be more effective in retrieving relevant code entities. We support this premise by presenting three approaches that lever-age structural information in code search. First, we present structure-based techniques to improve ranking in retrieving implementations of commonly sought for programming features, where our best technique outperforms Google and Google Code Search. Second, we present Test-Driven Code Search (TDCS), an approach to finding reusable code fragments on the Internet, that uses structure-based code retrieval and dependency slicing – a technique to automatically pull code dependencies. Evaluation of TDCS with 34 students shows that TDCS is the fastest approach to find reusable code fragments for 59% of the students, and faster than Google Code Search for 66% of the students. Finally, we present Structural Semantic Indexing, a technique to associate meaningful terms with source code entities that improves the performance of retrieving code fragments to be used as API usage examples.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781124207803},
year = {2010},
title = {Facilitating internet-scale code retrieval},
language = {eng},
author = {Bajracharya, Sushil},
keywords = {Information Science ; Computer Science ; Information Science ; Computer Science ; Communication and the Arts ; Applied Sciences ; Code Retrieval ; Code Search Engines ; Information Retrieval ; Software Engineering ; Structural Semantic Indexing},
url = {http://search.proquest.com/docview/751931865/},
}

@misc{ShepherdDavid2007Nlpa,
abstract = {Because software systems are large and complex, developers often use software tools to understand unfamiliar code. In turn, software tools often utilize information about the program in the form of various program representations, which can provide detailed program information. Because traditional program representations do not capture the natural language clues in code, they often fail to assist the developer during high-level program understanding tasks. To bridge the gap between current software tools and the software developers' high-level questions, we propose supplementing traditional program representations with a natural language representation that exploits the information embedded in the program's names and comments. Any software tool that uses a program representation must automatically construct that representation. To automatically construct our natural language program representation, we combined natural language processing and traditional program analysis techniques. With these techniques, we extract the natural language clues from the method names, class names, and comments in a program. We evaluated the usefulness of our natural language program representation by developing two software tools which access our representation. The first, a software search tool called Find-Concept, locates code segments relevant to a developer's query, a common first step in development tasks. In a user study, Find-Concept found code segments more effectively and more consistently than a state-of-the-art information retrieval search tool and a lexical search tool. The second, an aspect mining tool called Timna, identifies code segments that could be more elegantly represented in a new language paradigm, aspect-oriented programming. Timna originally only used traditional program representations, yet when we added our natural language program representation, Timna was much more effective. In the software tools' respective evaluations, both tools performed well. The body of quantitative and qualitative evidence strongly suggests that supplementing traditional program representations with a natural language representation is a promising approach to increasing the effectiveness of software tools.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780549186588},
year = {2007},
title = {Natural language program analysis: Combining natural language processing with program analysis to improve software maintenance tools},
language = {eng},
author = {Shepherd, David},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Aspect Mining ; Code Search ; Natural Language ; Natural Language Processing ; Program Analysis ; Software Maintenance},
url = {http://search.proquest.com/docview/304862704/},
}

@misc{PoshyvanykDenys2008Uirt,
abstract = {Software is comprised of a multitude of artifacts; some of them are intended to be read by the compiler, while many others are intended to be read by developers. During software evolution developers have to maintain large software, often written by others. The user centric information is often expressed in natural language and it is embedded in documentation and source code. External documentation written in natural language (e.g., requirements, design documents, user manuals, etc.), the comments, and the identifiers encode to a large degree the domain of the software and capture design decisions, change requests, developer information, etc. This mainly unstructured information is usually larger in size than the source code. Storing and sharing this information is much needed today, when most development teams are distributed geographically and change frequently over time. Given the large amount of textual data present in existing software systems, tools are necessary for its storage, retrieval, and analysis, before it is delivered to the users. The dissertation proposes and validates the use of Information Retrieval techniques to extract and represent the textual information in large scale software systems such that it can be automatically combined with structural information to better support a variety of software maintenance tasks and activities. Specifically, the research focuses on combining textual information extracted with Information Retrieval methods with program dependencies, execution traces and other analyses to define new techniques for feature location and new measures for class cohesion and coupling. The main contributions of this dissertation work are as the following: (1) Definition and validation of a novel approach to concept location in source code, which combines Formal Concept Analysis and Latent Semantic Indexing. (2) Definition and validation of a novel approach for feature location, which relies on the combination of probabilistic ranking of methods based on execution scenarios and Information Retrieval. (3) Definition and validation of a semi-automated technique for feature location in source code, based on the combination of a single execution trace and the comments and identifiers from the source code. (4) Definition and validation of novel measures for cohesion and coupling of classes in Object-Oriented systems, based on the analysis of textual information in software. We expect that these new techniques and software measures will contribute directly to the improvement of the design of incremental change of software and thus will lead to the increased software quality and reduced software maintenance costs.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780549598602},
year = {2008},
title = {Using information retrieval to support software maintenance tasks},
language = {eng},
author = {Poshyvanyk, Denys},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Feature Location ; Information Retrieval ; Program Comprehension ; Software Maintenance ; Software Measurement},
url = {http://search.proquest.com/docview/304446260/},
}

@inproceedings{GrechanikMark2010EEeA,
series = {ICSE '10},
abstract = {<p><p>Searching for applications that are highly relevant to development tasks is challenging because the high-level intent reflected in the descriptions of these tasks doesn't usually match the low-level implementation details of applications. In this demo we show a novel code search engine called <i>Exemplar (EXEcutable exaMPLes ARchive)</i> to bridge this mismatch. Exemplar takes natural-language query that contains high-level concepts (e.g., MIME, data sets) as input, then uses information retrieval and program analysis techniques to retrieve applications that implement these concepts.</p></p>},
pages = {259--262},
volume = {2},
publisher = {ACM},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on software engineering},
isbn = {9781605587196},
year = {2010},
title = {Exemplar: EXEcutable exaMPLes ARchive},
language = {eng},
author = {Grechanik, Mark and Fu, Chen and Xie, Qing and Mcmillan, Collin and Poshyvanyk, Denys and Cumby, Chad},
}

@inproceedings{WangYaojing2015DBFb,
series = {Internetware '15},
abstract = {<p><p>During an Open Source Software (OSS) maintenance, bug localization is a laborsome and time-consuming work for geographically-separated developers. It is desirable to automatically identify related buggy files once a bug report is submitted. Existing work has proposed information retrieval techniques for this problem. However, these proposals tend to neglect the inherent structure in bug localization and they are largely dependent on the comments (annotations) in source files which may be unavailable. In this paper, we propose a random walk based approach to detecting buggy source files based on bug reports. In particular, we separately process source files and bug reports to make it less sensitive to comments, and then apply random walk to capture the inherent structure in bug localization. Experimental evaluations on three real-world open-source projects demonstrate that the proposed approach can outperform several existing methods and that it is less dependent on the comments in source files.</p></p>},
pages = {62--69},
volume = {06-},
publisher = {ACM},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on internetware},
isbn = {9781450336413},
year = {2015},
title = {Detecting Buggy Files based on Bug Reports: A Random Walk Based Approach},
language = {eng},
author = {Wang, Yaojing and Xu, Feng and Yao, Yuan and Wu, Yong},
keywords = {Bug Localization ; Bug Reports ; Open Source Software ; Random Walk},
}

@article{GharibiReza2018Ltpo,
issn = {0306-4573},
abstract = {•A multi-component bug localization approach that leverages different textual properties of bug reports and source files.•The approach utilizes information retrieval, textual matching, stack trace analysis, and multi-label classification.•It is applied to real-world software projects and achieved good performance.•It Improves the MRR and MAP metrics compared to several existing state-of-the-art approaches. Bug reports are an essential part of a software project's life cycle since resolving them improves the project's quality. When a new bug report is received, developers usually need to reproduce the bug and perform code review to locate the bug and assign it to be fixed. However, the huge number of bug reports and the increasing size of software projects make this process tedious and time-consuming. To solve this issue, bug localization techniques try to rank all the source files of a project with respect to how likely they are to contain a bug. This process reduces the search space of source files and helps developers to find relevant source files quicker. In this paper, we propose a multi-component bug localization approach that leverages different textual properties of bug reports and source files as well as the relations between previously fixed bug reports and a newly received one. Our approach uses information retrieval, textual matching, stack trace analysis, and multi-label classification to improve the performance of bug localization. We evaluate the performance of the proposed approach on three open source software projects (i.e., AspectJ, SWT, and ZXing) and the results show that it can rank appropriate source files for more than 52% of bugs by recommending only one source file and 78% by recommending ten files. It also improves the MRR and MAP values compared to several existing state-of-the-art bug localization approaches.},
journal = {Information Processing and Management},
pages = {1058--1076},
volume = {54},
publisher = {Elsevier Ltd},
number = {6},
year = {2018},
title = {Leveraging textual properties of bug reports to localize relevant source files},
language = {eng},
author = {Gharibi, Reza and Rasekh, Amir Hossein and Sadreddini, Mohammad Hadi and Fakhrahmad, Seyed Mostafa},
keywords = {Bug Localization ; Bug Report ; Classification ; Information Retrieval ; Textual Analysis},
}

@article{DitBogdan2013Iire,
issn = {1382-3256},
abstract = {Data fusion is the process of integrating multiple sources of information such that their combination yields better results than if the data sources are used individually. This paper applies the idea of data fusion to feature location, the process of identifying the source code that implements specific functionality in software. A data fusion model for feature location is presented which defines new feature location techniques based on combining information from textual, dynamic, and web mining or link analyses algorithms applied to software. A novel contribution of the proposed model is the use of advanced web mining algorithms to analyze execution information during feature location. The results of an extensive evaluation on three Java systems indicate that the new feature location techniques based on web mining improve the effectiveness of existing approaches by as much as 87%.},
journal = {Empirical Software Engineering},
pages = {277--309},
volume = {18},
publisher = {Springer US},
number = {2},
year = {2013},
title = {Integrating information retrieval, execution and link analysis algorithms to improve feature location in software},
language = {eng},
address = {Boston},
author = {Dit, Bogdan and Revelle, Meghan and Poshyvanyk, Denys},
keywords = {Concept location ; Feature identification ; Information retrieval ; Web mining ; Program comprehension ; Software evolution and maintenance},
}

@misc{CorleyChristopher2018OTMf,
abstract = {Topic modeling is a machine learning technique for discovering thematic structure within a corpus. Topic models have been applied to several areas of software engineering, including bug localization, feature location, triaging change requests, and traceability link recovery. Many of these approaches train topic models on a source code snapshot – a revision or state of code at a particular point of time, such as a versioned release. However, source code evolution leads to model obsolescence and thus to the need to retrain the model from the latest snapshot, incurring a non-trivial computational cost of model re-learning. This work proposes and investigates an approach that can remedy the obsolescence problem. Conventional wisdom in the software maintenance research community holds that the topic model training information must be the same information that is of interest for retrieval. The primary insight for this work is that topic models can infer the topics of any information, regardless of the information used to train the model. Pairing online topic modeling with mining software repositories, I can remove the need to retrain a model and achieve model persistence. For this, I suggest training of topic models on the software repository history in the form of the changeset – a textual representation of the changes that occur between two source code snapshots. To show the feasibility of this approach, I investigate two popular applications of text retrieval in software maintenance, feature location and developer identification. Feature location is a search activity for locating the source code entity that relates to a feature of interest. Developer identification is similar, but focuses on identifying the developer most apt for working on a feature of interest. Further, to demonstrate the usability of changeset-based topic models, I investigate whether I can coalesce topic-modeling-based maintenance tasks into using a single model, rather than needing to train a model for each task at hand. In sum, this work aims to show that training online topic models on software repositories removes retraining costs while maintaining accuracy of a traditional snapshot-based topic model for different software maintenance problems.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9780438040977},
year = {2018},
title = {Online Topic Modeling for Software Maintenance Using a Changeset-Based Approach},
language = {eng},
author = {Corley, Christopher},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Changesets ; Developer Identification ; Feature Location ; Mining Software Repositories ; Online Topic Modeling ; Program Comprehension},
url = {http://search.proquest.com/docview/2055269292/},
}

@misc{AlhindawiNouh2013Sscc,
abstract = {This dissertation addresses the problems of program comprehension to support the evolution of large-scale software systems. The research concerns how software engineers locate features and concepts along with categorizing changes within very large bodies of source code along with their versioned histories. More specifically, advanced Information Retrieval (IR) and Natural Language Processing (NLP) are utilized and enhanced to support various software engineering tasks. This research is not aimed at directly improving IR or NLP approaches; rather it is aimed at understanding how additional information can be leveraged to improve the final results. The work advances the field by investigating approaches to augment and re-document source code with different types of abstract behavior information. The hypothesis is that enriching the source code corpus with meaningful descriptive information, and integrating this orthogonal information (semantic and structural) that is extracted from source code, will improve the results of the IR methods for indexing and querying information. Moreover, adding this new information to a corpus is a form of supervision. That is, apriori knowledge is often used to direct and supervise machine-learning and IR approaches. The main contributions of this dissertation involve improving on the results of previous work in feature location and source code querying. The dissertation demonstrates that the addition of statically derived information from source code (e.g., method stereotypes) can improve the results of IR methods applied to the problem of feature location. Further contributions include showing the effects of eliminating certain textual information (comments and function calls) from being included when performing source code indexing for feature/concept location. Moreover, the dissertation demonstrates an IR-based method of natural language topic extraction that assists developers in gaining an overview of past maintenance activities based on software repository commits. The ultimate goal of this work is to reduce the costs, effort, and time of software maintenance by improving the results of previous work in feature location and source code querying, and by supporting a new platform for enhancing program comprehension and facilitating software engineering research.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781303874970},
year = {2013},
title = {Supporting source code comprehension during software evolution and maintenance},
language = {eng},
author = {Alhindawi, Nouh},
keywords = {Information Technology ; Computer Science ; Information Technology ; Computer Science ; Applied Sciences ; Information Retrieval ; Latent Semantic Indexing ; Method Stereotypes ; Software Comprehension ; Software Evolution ; Software Maintenance},
url = {http://search.proquest.com/docview/1531717844/},
}

@article{GeXi2017Daeo,
issn = {1045-926X},
abstract = {Searching for relevant code in the local code base is a common activity during software maintenance. However, previous research indicates that 88% of manually composed search queries retrieve no relevant results. One reason that many searches fail is existing search tools’ dependence on string matching algorithms, which cannot find semantically related code. To solve this problem by helping developers compose better queries, researchers have proposed numerous query recommendation techniques, relying on a variety of dictionaries and algorithms. However, few of these techniques are empirically evaluated by usage data from real-world developers. To fill this gap, we designed a multi-recommendation system that relies on the cooperation between several query recommendation techniques. We implemented and deployed this recommendation system within the Sando code search tool and conducted a longitudinal field study. Our study shows that over 34% of all queries were adopted from recommendation; and recommended queries retrieved results 11% more often than manual queries.},
journal = {Journal of Visual Languages and Computing},
pages = {1--9},
volume = {39},
publisher = {Elsevier Ltd},
number = {C},
year = {2017},
title = {Design and evaluation of a multi-recommendation system for local code search},
language = {eng},
author = {Ge, Xi and Shepherd, David C. and Damevski, Kostadin and Murphy-Hill, Emerson},
keywords = {Code Search ; Recommender Systems ; Field Study},
}

@article{BinkleyDave2018Tnfs,
issn = {1382-3256},
abstract = {For over two decades, software engineering (SE) researchers have been importing tools and techniques from information retrieval (IR). Initial results have been quite positive. For example, when applied to problems such as feature location or re-establishing traceability links, IR techniques work well on their own, and often even better in combination with more traditional source code analysis techniques such as static and dynamic analysis. However, recently there has been growing awareness among SE researchers that IR tools and techniques are designed to work under different assumptions than those that hold for a software system. Thus it may be beneficial to consider IR-inspired tools and techniques that are specifically designed to work with software. One aim of this work is to provide quantitative empirical evidence in support of this observation. To do so a new technique is introduced that captures the level of difficulty found in an information need , the true, often latent, information that a searcher desires to know. The new technique is used to compare two domains : Natural Language (NL) and SE. Analysis of the data leads to three significant findings. First, the variation in the distribution of difficulty of the SE information needs differs from that of the NL information needs; second, collection age plays a role in the differences between the NL collections; and finally, the retrieval model used has little impact on the results.},
journal = {Empirical Software Engineering},
pages = {2398--2425},
volume = {23},
publisher = {Springer US},
number = {4},
year = {2018},
title = {The need for software specific natural language techniques},
language = {eng},
address = {New York},
author = {Binkley, Dave and Lawrie, Dawn and Morrell, Christopher},
keywords = {Feature location ; Information retrieval ; Query quality ; Information need ; Test collection challenge},
}

@misc{RaoShivani2013Iuff,
abstract = {Information Retrieval (IR) based bug localization techniques use a bug report to query a software repository to retrieve relevant source files. Search is performed using a query that is constructed from the bug report on an index built from the the source files in the software repository. Much of the current research is focused on improving the retrieval effectiveness of these methods. However, little consideration has been given to the efficiency of such approaches for software repositories that are constantly evolving. As a software repository evolves, the index creation and model learning have to be repeated to ensure accurate retrieval for each new bug. This amounts to redundant computations for the vast majority of the source files that remain unchanged in a single commit thereby increasing the retrieval time for a given query. To address these issues, we propose an incremental update framework that continuously updates the index and the model on the basis of just the changes made at each commit. We demonstrate the versatility of our framework using four popular text models --- Vector Space Model (VSM), Smoothed Unigram Model (SUM), Latent Semantic Analysis (LSA), and Latent Dirichlet Allocation (LDA). We show that the same retrieval accuracy can be achieved but with a fraction of the time needed by current approaches. We also propose strategies to identify commits where the index and the model may require to be re-computed. The dataset we used in our validation experiments was created by tracking the commit history of AspectJ and JodaTime software libraries over a span of 10 years.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781303761386},
year = {2013},
title = {Incremental update framework for efficient retrieval from software libraries for bug localization},
language = {eng},
author = {Rao, Shivani},
keywords = {Computer Engineering ; Computer Engineering ; Applied Sciences ; Bug Localization ; Information Retrieval ; Software Engineering},
url = {http://search.proquest.com/docview/1511456477/},
}

@article{MillsChris2017PQQf,
issn = {1557-7392},
abstract = {<p><p><b>Context:</b> Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts.</p> <p><b>Aim:</b> We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself.</p> <p><b>Method:</b> We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery.</p> <p><b>Results:</b> For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average.</p> <p><b>Conclusions:</b> The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.</p></p>},
journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
pages = {1--45},
volume = {26},
publisher = {ACM},
number = {1},
year = {2017},
title = {Predicting Query Quality for Applications of Text Retrieval to Software Engineering Tasks},
language = {eng},
author = {Mills, Chris and Bavota, Gabriele and Haiduc, Sonia and Oliveto, Rocco and Marcus, Andrian and Lucia, Andrea},
keywords = {Text Retrieval ; Artifact Traceability ; Concept Location ; Computer Science},
}

@article{RaoShivani2015CILS,
issn = {0163-5948},
abstract = {<p><p>The problem of bug localization is to identify the source files related to a bug in a software repository. Information Retrieval (IR) based approaches create an index of the source files and learn a model which is then queried with a bug for the relevant files. In spite of the advances in these tools, the current approaches do not take into consideration the dynamic nature of software repositories. With the traditional IR based approaches to bug localization, the model parameters must be recalculated for each change to a repository. In contrast, this paper presents an incremental framework to update the model parameters of the Latent Semantic Analysis (LSA) model as the data evolves. We compare two state-of-the-art incremental SVD update techniques for LSA with respect to the retrieval accuracy and the time performance. The dataset we used in our validation experiments was created from mining 10 years of version history of AspectJ and JodaTime software libraries.</p></p>},
journal = {ACM SIGSOFT Software Engineering Notes},
pages = {1--8},
volume = {40},
publisher = {ACM},
number = {1},
year = {2015},
title = {Comparing Incremental Latent Semantic Analysis Algorithms for Efficient Retrieval from Software Libraries for Bug Localization},
language = {eng},
author = {Rao, Shivani and Medeiros, Henry and Kak, Avinash},
keywords = {Bug Localization ; Incremental Learning ; Information Retrieval ; Latent Semantic Analysis ; Singular Value Decomposition ; Computer Science},
}

@article{BinkleyDave2015EiIf,
issn = {0164-1212},
abstract = {Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of IR requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the SEMERU dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as LSI and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100% better than the average.},
journal = {The Journal of Systems & Software},
pages = {30--42},
volume = {101},
publisher = {Elsevier Inc.},
number = {C},
year = {2015},
title = {Enabling improved IR-based feature location},
language = {eng},
author = {Binkley, Dave and Lawrie, Dawn and Uehlinger, Christopher and Heinz, Daniel},
keywords = {Information Retrieval Models ; Query Formulation ; Feature Location},
}

@article{DamevskiKostadin2016Afso,
issn = {1382-3256},
abstract = {Our current understanding of how programmers perform feature location during software maintenance is based on controlled studies or interviews, which are inherently limited in size, scope and realism. Replicating controlled studies in the field can both explore the findings of these studies in wider contexts and study new factors that have not been previously encountered in the laboratory setting. In this paper, we report on a field study about how software developers perform feature location within source code during their daily development activities. Our study is based on two complementary field data sets: one that reflects complete IDE activity of 67 professional developers over approximately one month, and the other that reflects usage of an IR-based code search tool by nearly 600 developers. Analyzing this data, we report results on how often developers use which type of code search tools, on the types of queries and retreival strategies used by developers, and on patterns of developer feature location behavior following code search. The results of the study suggest that there is (1) a need for helping developers to devise better code search queries; (2) a lack of adoption of niche code search tools; (3) a need for code search tool to handle both lookup and exploratory queries; and (4) a need for better integration between code search, structured navigation, and debugging tools in feature location tasks.},
journal = {Empirical Software Engineering},
pages = {724--747},
volume = {21},
publisher = {Springer US},
number = {2},
year = {2016},
title = {A field study of how developers locate features in source code},
language = {eng},
address = {New York},
author = {Damevski, Kostadin and Shepherd, David and Pollock, Lori},
keywords = {Code search ; Feature location ; Field studies},
}

@misc{SismanBunyamin2013Scrf,
abstract = {This dissertation advances the state-of-the-art in information retrieval (IR) based approaches to automatic bug localization in software. In an IR-based approach, one first creates a search engine using a probabilistic or a deterministic model for the files in a software library. Subsequently, a bug report is treated as a query to the search engine for retrieving the files relevant to the bug. With regard to the new work presented, we first demonstrate the importance of taking version histories of the files into account for achieving significant improvements in the precision with which the files related to a bug are located. This is motivated by the realization that the files that have not changed in a long time are likely to have ``stabilized" and are therefore less likely to contain bugs. Subsequently, we look at the difficulties created by the fact that developers frequently use abbreviations and concatenations that are not likely to be familiar to someone trying to locate the files related to a bug. We show how an initial query can be automatically reformulated to include the relevant actual terms in the files by an analysis of the files retrieved in response to the original query for terms that are proximal to the original query terms. The last part of this dissertation generalizes our term-proximity based work by using Markov Random Fields (MRF) to model the inter-term dependencies in a query vis-a-vis the files. Our MRF work redresses one of the major defects of the most commonly used modeling approaches in IR, which is the loss of all inter-term relationships in the documents.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781303762741},
year = {2013},
title = {Source code retrieval from large software libraries for automatic bug localization},
language = {eng},
author = {Sisman, Bunyamin},
keywords = {Computer Engineering ; Computer Engineering ; Applied Sciences ; Bug Localization ; Information Retrieval ; Software Maintenance ; Source Code Retrieval},
url = {http://search.proquest.com/docview/1511677664/},
}

@article{XuBowen2018Dcrq,
issn = {1382-3256},
abstract = {Chinese developers often cannot effectively search questions in English, because they may have difficulties in translating technical words from Chinese to English and formulating proper English queries. For the purpose of helping Chinese developers take advantage of the rich knowledge base of Stack Overflow and simplify the question retrieval process, we propose an automated cross-language relevant question retrieval ( CLRQR ) system to retrieve relevant English questions for a given Chinese question. CLRQR first extracts essential information (both Chinese and English) from the title and description of the input Chinese question, then performs domain-specific translation of the essential Chinese information into English, and finally formulates an English query for retrieving relevant questions in a repository of English questions from Stack Overflow. We propose three different retrieval algorithms (word-embedding, word-matching, and vector-space-model based methods) that exploit different document representations and similarity metrics for question retrieval. To evaluate the performance of our approach and investigate the effectiveness of different retrieval algorithms, we propose four baseline approaches based on the combination of different sources of query words, query formulation mechanisms and search engines. We randomly select 80 Java, 20 Python and 20 .NET questions in SegmentFault and V2EX (two Chinese Q&A websites for computer programming) as the query Chinese questions. We conduct a user study to evaluate the relevance of the retrieved English questions using CLRQR with different retrieval algorithms and the four baseline approaches. The experiment results show that CLRQR with word-embedding based retrieval achieves the best performance.},
journal = {Empirical Software Engineering},
pages = {1084--1122},
volume = {23},
publisher = {Springer US},
number = {2},
year = {2018},
title = {Domain-specific cross-language relevant question retrieval},
language = {eng},
address = {New York},
author = {Xu, Bowen and Xing, Zhenchang and Xia, Xin and Lo, David and Li, Shanping},
keywords = {Domain-specific translation ; Cross-language question retrieval},
}

@article{SirresRaphael2018Aasu,
issn = {1382-3256},
abstract = {Source code terms such as method names and variable types are often different from conceptual words mentioned in a search query. This vocabulary mismatch problem can make code search inefficient. In this paper, we present COde voCABUlary ( CoCaBu ), an approach to resolving the vocabulary mismatch problem when dealing with free-form code search queries. Our approach leverages common developer questions and the associated expert answers to augment user queries with the relevant, but missing, structural code entities in order to improve the performance of matching relevant code examples within large code repositories. To instantiate this approach, we build GitSearch , a code search engine, on top of GitHub and Stack Overflow Q&A data. We evaluate GitSearch in several dimensions to demonstrate that (1) its code search results are correct with respect to user-accepted answers; (2) the results are qualitatively better than those of existing Internet-scale code search engines ; (3) our engine is competitive against web search engines , such as Google, in helping users solve programming tasks; and (4) GitSearch provides code examples that are acceptable or interesting to the community as answers for Stack Overflow questions.},
journal = {Empirical Software Engineering},
pages = {2622--2654},
volume = {23},
publisher = {Springer US},
number = {5},
year = {2018},
title = {Augmenting and structuring user queries to support efficient free-form code search},
language = {eng},
address = {New York},
author = {Sirres, Raphael and Bissyandé, Tegawendé and Kim, Dongsun and Lo, David and Klein, Jacques and Kim, Kisub and Traon, Yves},
keywords = {Code search ; GitHub ; Free-form search ; Query augmentation ; StackOverflow ; Vocabulary mismatch},
}

@article{2018RSSR,
issn = {1944-1592},
journal = {Computer Weekly News},
publisher = {NewsRX LLC},
year = {2018},
title = {Reports Summarize Software Research Findings from Wuhan University (Query expansion based on statistical learning from code changes).(Report)},
language = {eng},
}

@inproceedings{HillEmily2009Acsc,
series = {ICSE '09},
abstract = {<p>As software systems continue to grow and evolve, locating code for maintenance and reuse tasks becomes increasingly difficult. Existing static code search techniques using natural language queries provide little support to help developers determine whether search results are relevant, and few recommend alternative words to help developers reformulate poor queries. In this paper, we present a novel approach that automatically extracts natural language phrases from source code identifiers and categorizes the phrases and search results in a hierarchy. Our contextual search approach allows developers to explore the word usage in a piece of software, helping them to quickly identify relevant program elements for investigation or to quickly recognize alternative words for query reformulation. An empirical evaluation of 22 developers reveals that our contextual search approach significantly outperforms the most closely related technique in terms of effort and effectiveness.</p>},
pages = {232--242},
publisher = {IEEE Computer Society},
booktitle = {Proceedings of the 31st International Conference on software engineering},
isbn = {9781424434534},
year = {2009},
title = {Automatically capturing source code context of NL-queries for software maintenance and reuse},
language = {eng},
author = {Hill, Emily and Pollock, Lori and Vijay-Shanker, K},
}

@inproceedings{BajracharyaSushil2010SAue,
series = {SUITE '10},
abstract = {<p><p>We present Sourcerer API Search (SAS), a search interface to find API usage examples in large code repositories. SAS facilitates finding API usage examples by providing three unique features: (i) code snippets view for each result that shows the portions of code where APIs are used; (ii) Tag-cloud view of popular words to facilitate query reformulation, and (iii) filtering results using APIs to narrow search results. Furthermore, SAS uses a code index where each code entity is indexed with terms not only found in the entity but also in other entities having similar API usage. These features make SAS a novel search interface to find API usage examples in code repositories.</p></p>},
pages = {5--8},
publisher = {ACM},
booktitle = {Proceedings of 2010 ICSE Workshop on search-driven development: users, infrastructure, tools and evaluation},
isbn = {9781605589626},
year = {2010},
title = {Searching API usage examples in code repositories with sourcerer API search},
language = {eng},
author = {Bajracharya, Sushil and Ossher, Joel and Lopes, Cristina},
keywords = {Api Search ; Exploratory Code Search ; Search Driven Development ; Search User Interface ; Software Information Retrieval},
}

@misc{HaiducSonia2013Strq,
abstract = {The text found in software artifacts captures important information. Text Retrieval (TR) techniques have been successfully used to leverage this information. Despite their advantages, the success of TR techniques strongly depends on the textual queries given as input. When poorly chosen queries are used, developers can waste time investigating irrelevant results. The quality of a query indicates the relevance of the results returned by TR in response to the query and can give an indication if the results are worth investigating or a reformulation of the query should be sought instead. Knowing the quality of the query could lead to time saved when irrelevant results are returned. However, the only way to determine if a query led to the wanted artifacts is by manually inspecting the list of results. This dissertation introduces novel approaches to measure and predict the quality of queries automatically in the context of SE tasks, based on a set of statistical properties of the queries. The approaches are evaluated for the task of concept location in source code. The results reveal that the proposed approaches are able to accurately capture and predict the quality of queries for SE tasks supported by TR. When a query has low quality, the developer can reformulate it and improve it. However, this is just as hard as formulating the query in the first place. This dissertation presents two approaches for partial and complete automation of the query reformulation process. The semi-automatic approach relies on developer feedback about the relevance of TR results and uses this information to automatically reformulate the query. The automatic approach learns and applies the best reformulation approach for a query and relies on a set of training queries and their statistical properties to achieve this. Both approaches are evaluated for concept location and the results show that the techniques are able to improve the results of the original queries in the majority of the cases. We expect that on the long run the proposed approaches will contribute directly to the reduction of developer effort and implicitly the reduction of software evolution costs.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781303393556},
year = {2013},
title = {Supporting text retrieval query formulation in software engineering},
language = {eng},
author = {Haiduc, Sonia},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Concept Location ; Query ; Query Quality ; Reformulation ; Software Evolution ; Text Retrieval},
url = {http://search.proquest.com/docview/1442862600/},
}

@inproceedings{ChaparroOscar2016Otro,
series = {ICSE '16},
abstract = {<p><p>We argue that verbose queries used for software retrieval contain many terms that follow specific discourse rules, yet hinder retrieval. We report the results of an empirical study on the effect of removing such terms from verbose queries in the context of Text Retrieval-based concept location. In the study, we remove terms from 424 queries, generated from bug reports of nine open source systems. Removing the terms leads to substantial improvement in retrieval: 73% of the queries are improved, leading to 21.8% and 13.4% gain in terms of MRR and MAP, respectively. Such improvement is larger than that of many more sophisticated state-of-the-art approaches. The results show promise and the future challenge lies with automatically identifying the terms to be removed from the verbose queries.</p></p>},
pages = {716--718},
publisher = {ACM},
booktitle = {Proceedings of the 38th International Conference on software engineering companion},
isbn = {9781450342056},
year = {2016},
title = {On the reduction of verbose queries in text retrieval based software maintenance},
language = {eng},
author = {Chaparro, Oscar and Marcus, Andrian},
keywords = {Query Reduction ; Software Maintenance ; Text Retrieval},
}

@inproceedings{TranD.H.2015EFrf,
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
pages = {143--154},
volume = {326},
publisher = {Springer Verlag},
isbn = {9783319116792},
year = {2015},
title = {Easysearch: Finding relevant functions based on API documentation},
copyright = {Copyright 2016 Elsevier B.V., All rights reserved.},
author = {Tran, D.H. and Nguyen, H.P. and Hanh Le, D.},
keywords = {Api Documentation ; Code Search ; Keyword Weighting ; Query Expansion ; Semantic Ranking},
}

@inproceedings{MartieL.2016CSro,
pages = {24--35},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
isbn = {9781509000241},
year = {2016},
title = {CodeExchange: Supporting reformulation of internet-scale code queries in context},
copyright = {Copyright 2016 Elsevier B.V., All rights reserved.},
author = {Martie, L. and Latoza, T.D. and Van Der Hoek, A.},
keywords = {Code Search ; Context ; Interface ; Internet-Scale ; Query Reformulation},
}

@inproceedings{ChaparroO.2017UOBt,
pages = {376--387},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
isbn = {9781538609927},
year = {2017},
title = {Using Observed Behavior to reformulate queries during text retrieval-based bug localization},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Chaparro, O. and Florez, J.M. and Marcus, A.},
keywords = {Bug Localization ; Observed Behavior ; Query Reformulation ; Text Retrieval},
}

@inproceedings{RahmanMohammad2017Rcsi,
series = {ICSE-C '17},
abstract = {<p><p>Traditional code search engines often do not perform well with natural language queries since they mostly apply keyword matching. These engines thus require carefully designed queries containing information about programming APIs for code search. Unfortunately, existing studies suggest that preparing an effective query for code search is both challenging and time consuming for the developers. In this paper, we propose a novel code search tool-RACK-that returns relevant source code for a given code search query written in natural language text. The tool first translates the query into a list of relevant API classes by mining keyword-API associations from the crowdsourced knowledge of Stack Overflow, and then applies the reformulated query to GitHub code search API for collecting relevant results. Once a query related to a programming task is submitted, the tool automatically mines relevant code snippets from thousands of open-source projects, and displays them as a ranked list within the context of the developer's programming environment-the IDE. Tool page: http://www.usask.ca/?masud.rahman/rack</p></p>},
pages = {51--54},
publisher = {IEEE Press},
booktitle = {Proceedings of the 39th International Conference on software engineering companion},
isbn = {9781538615898},
year = {2017},
title = {RACK: code search in the IDE using crowdsourced knowledge},
language = {eng},
author = {Rahman, Mohammad and Roy, Chanchal and Lo, David},
keywords = {Code Search ; Crowdsourced Knowledge ; Keyword-Api Association ; Query Reformulation ; Stack Overflow},
}

@inproceedings{HaiducS.2013Qqpa,
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {1307--1310},
isbn = {9781467330763},
year = {2013},
title = {Query quality prediction and reformulation for source code search: The Refoqus tool},
copyright = {Copyright 2013 Elsevier B.V., All rights reserved.},
author = {Haiduc, S. and De Rosa, G. and Bavota, G. and Oliveto, R. and De Lucia, A. and Marcus, A.},
keywords = {Query Quality ; Query Reformulation ; Source Code Search ; Text Retrieval},
}

@inproceedings{SatterA.2017Aslm,
pages = {586--591},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {19th International Conference on Computer and Information Technology, ICCIT 2016},
isbn = {9781509040896},
year = {2017},
title = {A search log mining based query expansion technique to improve effectiveness in code search},
copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
author = {Satter, A. and Sakib, K.},
keywords = {Code Reuse ; Code Search ; Term Mismatch},
}

@misc{NiuHaoran2015ICSU,
abstract = {During the process of software development, developers often encounter unfamiliar programming tasks. Online Q&A forums, such as StackOverflow, are one of the resources that developers can ask for answers to their programming questions. Automatic recommendation of a working code example can be helpful to solve developers' programming questions. However, existing code search engines support mainly keyword-based queries, and do not accomodate well natural-language code search queries. Specifically, natural-language queries contain less technical keywords, i.e., class or method names, which negatively affects the success of the code search process of existing code search engines. On the other hand, a code search engine requires a ranking schema to place relevant code examples at the top of the result list. How- ever, existing ranking schemas are hand-crafted heuristics where the configurations are hard to determine, which leads to the difficulty in using them for new languages or frameworks. In this paper, we propose the approach which uses query reformulation techniques to improve the search effectiveness of existing code search engines for natural- language queries. The approach automatically reformulate natural-language queries using class-names with semantic relations. We also propose an approach to automatically train a ranking schema for the code example search using the learning-to-rank technique. We evaluate the proposed approaches using a large-scale corpus of code examples. The evaluation results show that our approaches can effectively recommend semantically related class-names to reformulate natural-language queries, and the improvement on the search effectiveness over existing query reformulation approaches is statistically significant. The automatically trained ranking schema can effectively rank code examples, and outperform the existing ranking schemas by 35.65% and 48.42% in terms of normalized discounted cumulative gain (NDCG) and expected reciprocal rank (ERR), respectively.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781369112528},
year = {2015},
title = {Improving Code Search Using Learning-to-Rank and Query Reformulation Techniques},
language = {eng},
author = {Niu, Haoran},
keywords = {Computer Engineering ; Information Technology ; Computer Engineering ; Information Technology ; Applied Sciences ; Code Search Engines ; Keyword-Based Queries ; Natural-Language Code ; Query},
url = {http://search.proquest.com/docview/1825293478/},
}

@inproceedings{RahmanM.M.2017Iqrf,
pages = {428--439},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
isbn = {9781538626849},
year = {2017},
title = {Improved query reformulation for concept location using CodeRank and document structures},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Rahman, M.M. and Roy, C.K.},
keywords = {Coderank ; Concept Location ; Data Resampling ; Query Quality Analysis ; Query Reformulation ; Term Weighting},
}

@inproceedings{HuangQ.2017SEtc,
issn = {23259000},
journal = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
pages = {307--312},
publisher = {Knowledge Systems Institute Graduate School},
isbn = {1891706411},
year = {2017},
title = {SnippetGen: Enhancing the code search via intent predicting},
copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
author = {Huang, Q. and Wang, X. and Yang, Y. and Wan, H. and Wang, R. and Wu, G.},
keywords = {Code Search ; Intent Predicting ; Query Expansion},
}

@inproceedings{LuM.2015QevW,
pages = {545--549},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
isbn = {9781479984695},
year = {2015},
title = {Query expansion via WordNet for effective code search},
copyright = {Copyright 2015 Elsevier B.V., All rights reserved.},
author = {Lu, M. and Sun, X. and Wang, S. and Lo, D. and Duan, Y.},
keywords = {Software ; Recall ; Query Processing ; Computer Programs ; Query Languages ; Source Code ; Developers ; Searching ; Software Engineering (General) (Ci);},
}

@inproceedings{LemosO.A.L.2015Ctuo,
pages = {41--50},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
booktitle = {2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings},
isbn = {9781467375290},
year = {2015},
title = {Can the use of types and query expansion help improve large-scale code search?},
copyright = {Copyright 2016 Elsevier B.V., All rights reserved.},
author = {Lemos, O.A.L. and De Paula, A.C. and Sajnani, H. and Lopes, C.V.},
keywords = {Code Search ; Query Expansion ; Software Reuse},
}

@inproceedings{LemosOtávioA.L2014Taqe,
series = {MSR 2014},
abstract = {<p><p> Software engineers often resort to code search practices to support software maintenance and evolution tasks, in particular code reuse. An issue that affects code search is the vocabulary mismatch problem: while searching for a particular function, users have to guess the exact words that were chosen by original developers to name code entities. In this paper we present an automatic query expansion (AQE) approach that uses word relations to increase the chances of finding relevant code. The approach is applied on top of Test-Driven Code Search (TDCS), a promising code retrieval technique that uses test cases as inputs to formulate the search query, but can also be used with other techniques that handle interface definitions to produce queries (interface-driven code search). Since these techniques rely on keywords and types, the vocabulary mismatch problem is also relevant. AQE is carried out by leveraging WordNet, a type thesaurus for expanding types, and another thesaurus containing only software-related word relations. Our approach is general but was specifically designed for non-native English speakers, who are frequently unaware of the most common terms used to name functions in software. Our evaluation with 36 non-native subjects - including developers and senior Computer Science students - provides evidence that our approach can improve the chances of finding relevant functions by 41% (recall improvement of 30%, on average), without hurting precision. </p></p>},
pages = {212--221},
publisher = {ACM},
booktitle = {Proceedings of the 11th Working Conference on mining software repositories},
isbn = {9781450328630},
year = {2014},
title = {Thesaurus-based automatic query expansion for interface-driven code search},
language = {eng},
author = {Lemos, Otávio A. L and de Paula, Adriano C and Zanichelli, Felipe C and Lopes, Cristina V},
keywords = {Automatic Query Expansion ; Code Search ; Software Reuse ; Computer Science},
}

@article{RahmanMohammadMasudur2016Qaqr,
series = {ASE 2016},
abstract = {<p><p> During maintenance, software developers deal with numerous change requests made by the users of a software system. Studies show that the developers find it challenging to select appropriate search terms from a change request during concept location. In this paper, we propose a novel technique--QUICKAR--that automatically suggests helpful reformulations for a given query by leveraging the crowdsourced knowledge from Stack Overflow. It determines semantic similarity or relevance between any two terms by analyzing their adjacent word lists from the programming questions of Stack Overflow, and then suggests semantically relevant queries for concept location. Experiments using 510 queries from two software systems suggest that our technique can improve or preserve the quality of 76% of the initial queries on average which is promising. Comparison with one baseline technique validates our preliminary findings, and also demonstrates the potential of our technique. </p></p>},
pages = {220--225},
publisher = {ACM},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on automated software engineering},
isbn = {9781450338455},
year = {2016},
title = {QUICKAR: automatic query reformulation for concept location using crowdsourced knowledge},
language = {eng},
author = {Rahman, Mohammad Masudur and Roy, Chanchal K},
keywords = {Query Reformulation ; Stack Overflow ; Adjacency List ; Crowdsourced Knowledge ; Semantic Relevance ; Word Co-Occurrence},
}

@article{RahmanM.M.2018PIbl,
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {348--349},
volume = {137351},
publisher = {IEEE Computer Society},
isbn = {9781450356633},
year = {2018},
title = {Poster: Improving bug localization with report quality dynamics and query reformulation},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Rahman, M.M. and Roy, C.K.},
keywords = {Computer Science - Software Engineering;},
}

@article{RahmanMohammadMasudur2018IQRf,
abstract = {During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique --ACER-- that takes an initial query, identifies appropriate search terms from the source code using a novel term weight --CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.},
year = {2018},
title = {Improved Query Reformulation for Concept Location using CodeRank and Document Structures},
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
keywords = {Computer Science - Software Engineering},
}

@article{RahmanMohammadMasudur2018IIBL,
abstract = {Recent findings suggest that Information Retrieval (IR)-based bug localization techniques do not perform well if the bug report lacks rich structured information (eg relevant program entity names). Conversely, excessive structured information (eg stack traces) in the bug report might not always help the automated localization either. In this paper, we propose a novel technique--BLIZZARD-- that automatically localizes buggy entities from project source using appropriate query reformulation and effective information retrieval. In particular, our technique determines whether there are excessive program entities or not in a bug report (query), and then applies appropriate reformulations to the query for bug localization. Experiments using 5,139 bug reports show that our technique can localize the buggy source documents with 7%--56% higher Hit@10, 6%--62% higher MAP@10 and 6%--62% higher MRR@10 than the baseline technique. Comparison with the state-of-the-art techniques and their variants report that our technique can improve 19% in MAP@10 and 20% in MRR@10 over the state-of-the-art, and can improve 59% of the noisy queries and 39% of the poor queries.},
year = {2018},
title = {Improving IR-Based Bug Localization with Context-Aware Query Reformulation},
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
keywords = {Computer Science - Software Engineering},
}

@article{RahmanMohammad2017Iqrf,
abstract = {During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique --ACER-- that takes an initial query, identifies appropriate search terms from the source code using a novel term weight --CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.},
journal = {PeerJ PrePrints},
publisher = {PeerJ, Inc.},
year = {2017},
title = {Improved query reformulation for concept location using CodeRank and document structures},
language = {eng},
address = {San Diego},
author = {Rahman, Mohammad and Roy, Chanchal},
keywords = {Computer Programs ; Queries ; Software ; Learning Algorithms ; Coderank ; Query Reformulation ; Query Quality Analysis ; Term Weighting ; Concept Location ; Data Resampling},
url = {http://search.proquest.com/docview/1952305482/},
}

@inproceedings{PérezF.2017Icfl,
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {114--131},
volume = {10573},
publisher = {Springer Verlag},
isbn = {9783319694610},
year = {2017},
title = {Introducing collaboration for locating features in models: Approach and industrial evaluation},
copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
author = {Pérez, F. and Marcén, A.C. and Lapeña, R. and Cetina, C.},
keywords = {Collaborative Information Retrieval ; Feature Location ; Model Driven Engineering ; Query Expansion},
}

@article{PérezFrancisca2018Aqrf,
issn = {0169-023X},
abstract = {No maintenance activity can be completed without Feature Location (FL), which is finding the set of software artifacts that realize a particular functionally. Despite the importance of FL, the vast majority of work has been focused on retrieving code, whereas other software artifacts such as the models have been neglected. Furthermore, locating a piece of information from a query in a large repository is a challenging task as it requires knowledge of the vocabulary used in the software artifacts. This can be alleviated by automatically reformulating the query (adding or removing terms). In this paper, we test four existing query reformulation techniques, which perform the best for FL in code but have never been used for FL in models. Specifically, we test these techniques in two industrial domains: a model-based family of firmwares for induction hobs, and a model-based family of PLC software to control trains. We compare the results provided by our FL approach using the query and the reformulated queries by means of statistical analysis. Our results show that reformulated queries do not improve the performance in models, which could lead towards a new direction in the creation or reconsideration of these techniques to be applied in models. •The influence of query reformulation in feature location for models is evaluated.•Similitude to a reformulated query guides a feature location evolutionary algorithm.•Reformulation fails to boost the quality of the solution in models as it does in code.},
journal = {Data & Knowledge Engineering},
pages = {159--176},
volume = {116},
publisher = {Elsevier B.V.},
year = {2018},
title = {Automatic query reformulations for feature location in a model-based family of software products},
language = {eng},
author = {Pérez, Francisca and Font, Jaime and Arcega, Lorena and Cetina, Carlos},
keywords = {Conceptual Modeling ; Information Retrieval ; Feature Location ; Query Reformulation ; Software Maintenance and Evolution ; Families of Software Products},
}

@article{BajracharyaSushil2012Aama,
issn = {1382-3256},
abstract = {This paper presents an analysis of a year long usage log of Koders, the first commercially available Internet-Scale code search engine ( http://www.koders.com ). The usage log comprises about ten million activities from more than three million users. Analysis of the usage data shows that despite of attracting a large number of visitors, Koders has a very sparse usage and that it lacks regular usage from many of its users. When compared to Web search, search behavior in Koders showed many similar patterns. A topic modeling analysis of the usage data shows what topics users of Koders are looking for. Observations on the prevalence of these topics among the users, and observations on how search and download activities vary across topics, lead to the conclusion that users who find code search engines usable are those who already know to a high level of specificity what to look for. This paper also presents a general categorization of these topics that provides insights on the different ways code search engine users express their queries. It identifies various forms of queries in Koders’s log and the kinds of results addressed by the queries. It also provides several suggestions for improvements in code search engines based on the analysis of usage, topics, and query forms. The work presented in this paper is the first of its kind that reveals several insights on the usage of an Internet-Scale code search engine.},
journal = {Empirical Software Engineering},
pages = {424--466},
volume = {17},
publisher = {Springer US},
number = {4},
year = {2012},
title = {Analyzing and mining a code search engine usage log},
language = {eng},
address = {Boston},
author = {Bajracharya, Sushil and Lopes, Cristina},
keywords = {Code search engine ; Usage log analysis ; Mining topics},
}

@article{NieL.2016QEBo,
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
pages = {771--783},
volume = {9},
publisher = {Institute of Electrical and Electronics Engineers},
number = {5},
year = {2016},
title = {Query Expansion Based on Crowd Knowledge for Code Search},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Nie, L. and Jiang, H. and Ren, Z. and Sun, Z. and Li, X.},
keywords = {Code Search ; Crowd Knowledge ; Information Retrieval ; Query Expansion ; Question &Amp; Answer Pair},
}

@article{HuangQ.2017QEvI,
issn = {02181940},
journal = {International Journal of Software Engineering and Knowledge Engineering},
pages = {1591--1601},
volume = {27},
publisher = {World Scientific Publishing Co. Pte Ltd},
number = {9-10},
year = {2017},
title = {Query Expansion via Intent Predicting},
copyright = {Copyright 2018 Elsevier B.V., All rights reserved.},
author = {Huang, Q. and Yang, Y. and Wang, X. and Wan, H. and Wang, R. and Wu, G.},
keywords = {Code Search ; Intent Prediction ; Query Expansion},
}

@article{HuangQing2018Qebo,
issn = {0038-0644},
abstract = {Thesaurus‐based, code‐related, and software‐specific query expansion techniques are the main contributions in free‐form query search. However, these techniques still could not put the most relevant query result in the first position because they lack the ability to infer the expansion words that represent the user needs based on a given query. In this paper, we discover that code changes can imply what users want and propose a novel query expansion technique with code changes (QECC). It exploits (changes, contexts) pairs from changed methods. On the basis of statistical learning from pairs, it can infer code changes for a given query. In this way, it expands a query with code changes and recommends the query results that meet actual needs perfectly. In addition, we implement InstaRec to perform QECC and evaluate it with 195 039 change commits from GitHub and our code tracker. The results show that QECC can improve the precision of 3 code search algorithms (ie, IR, Portfolio, and VF) by up to 52% to 62% and outperform the state‐of‐the‐art query expansion techniques (ie, query expansion based on crowd knowledge and CodeHow) by 13% to 16% when the top 1 result is inspected.},
journal = {Software: Practice and Experience},
pages = {1333--1351},
volume = {48},
number = {7},
year = {2018},
title = {Query expansion based on statistical learning from code changes},
author = {Huang, Qing and Yang, Yangrui and Zhan, Xue and Wan, Hongyan and Wu, Guoqing},
keywords = {Code Changes ; Code Search ; Information Retrieval ; Software Reuse ; Statistical Learning ; Query Expansion},
}

@inproceedings{SismanB.2013Acsw,
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
pages = {309--318},
isbn = {9781467329361},
year = {2013},
title = {Assisting code search with automatic query reformulation for bug localization},
copyright = {Copyright 2013 Elsevier B.V., All rights reserved.},
author = {Sisman, B. and Kak, A.C.},
keywords = {Bug Localization ; Pseudo Relevance Feedback ; Query Expansion ; Query Reformulation ; Software Maintenance},
}

@misc{ShaoPeng2011Cirm,
abstract = {Bug localization and feature location in source code are software evolution tasks in which developers use information about a bug or feature present in a software system to locate the source code elements, such as classes or methods. These classes or methods must be modified either to correct the bug or implement a feature. Automating bug localization and feature location are necessary due to the size and complexity of modern software systems. Recently, researchers have developed static bug localization and feature location techniques using information retrieval techniques, such as latent semantic indexing (LSI), to model lexical information, such as identifiers and comments, from source code. This research presents a new technique, LSICG, which combines LSI modeling lexical information and call graphs to modeling structural information. The output is a list of methods ranked in descending order by likelihood of requiring modification to correct the bug or implement the feature under consideration. Three case studies including comparison of LSI and LSICG at method level and class level of granularity on 25 features in JavaHMO, 35 bugs in Rhino, 3 features and 6 bugs in jEdit demonstrate that The LSICG technique provides improved performance compared to LSI alone.},
publisher = {ProQuest Dissertations Publishing},
isbn = {9781124963877},
year = {2011},
title = {Combining information retrieval modules and structural information for source code bug localization and feature location},
language = {eng},
author = {Shao, Peng},
keywords = {Computer Science ; Computer Science ; Applied Sciences ; Bug Localization ; Call Graph ; Feature Location ; Information Retrieval ; Latent Semantic Indexing},
url = {http://search.proquest.com/docview/903974668/},
}

@article{LiXiaoke2012Esom,
issn = {10226680},
journal = {Advanced Materials Research},
pages = {383--386},
volume = {450-451},
isbn = {9783037853481},
year = {2012},
title = {Experimental study on mix proportion of fair-faced concrete for urban bridge},
copyright = {Copyright 2012 Elsevier B.V., All rights reserved.},
author = {Li, Xiaoke and Ma, Xiaolu and Zhang, Xiaoyan},
keywords = {Apparent Quality ; Fair-Faced Concrete ; Image Analysis ; Mix Proportion ; Parting Agent ; Urban Bridge},
}

@article{2013AMR,
issn = {10226680},
volume = {665},
publisher = {Trans Tech Publications Ltd},
isbn = {9783037856253},
year = {2013},
title = {Advanced Materials Research},
copyright = {Copyright 2015 Elsevier B.V., All rights reserved.},
}

